<?xml version="1.0" ?>
<!DOCTYPE pmc-articleset PUBLIC "-//NLM//DTD ARTICLE SET 2.0//EN" "https://dtd.nlm.nih.gov/ncbi/pmc/articleset/nlm-articleset-2.0.dtd">
<pmc-articleset><article xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="research-article">
  <?properties open_access?>
  <front>
    <journal-meta>
      <journal-id journal-id-type="nlm-ta">ScientificWorldJournal</journal-id>
      <journal-id journal-id-type="iso-abbrev">ScientificWorldJournal</journal-id>
      <journal-id journal-id-type="publisher-id">TSWJ</journal-id>
      <journal-title-group>
        <journal-title>The Scientific World Journal</journal-title>
      </journal-title-group>
      <issn pub-type="epub">1537-744X</issn>
      <publisher>
        <publisher-name>Hindawi Publishing Corporation</publisher-name>
      </publisher>
    </journal-meta>
    <article-meta>
      <article-id pub-id-type="pmid">23861651</article-id>
      <article-id pub-id-type="pmc">3703905</article-id>
      <article-id pub-id-type="doi">10.1155/2013/206734</article-id>
      <article-categories>
        <subj-group subj-group-type="heading">
          <subject>Research Article</subject>
        </subj-group>
      </article-categories>
      <title-group>
        <article-title>Monitor, a Vibrotactile Aid for Environmental Perception: A Field Evaluation by Four People with Severe Hearing and Vision Impairment</article-title>
      </title-group>
      <contrib-group>
        <contrib contrib-type="author">
          <contrib-id contrib-id-type="orcid">0000-0003-4130-7909</contrib-id>
          <name>
            <surname>Ranjbar</surname>
            <given-names>Parivash</given-names>
          </name>
          <xref ref-type="aff" rid="I1">
<sup>1</sup>
</xref>
          <xref ref-type="aff" rid="I2">
<sup>2</sup>
</xref>
          <xref ref-type="aff" rid="I3">
<sup>3</sup>
</xref>
          <xref ref-type="corresp" rid="cor1">*</xref>
        </contrib>
        <contrib contrib-type="author">
          <name>
            <surname>Stenstr&#xF6;m</surname>
            <given-names>Ingeborg</given-names>
          </name>
          <xref ref-type="aff" rid="I4">
<sup>4</sup>
</xref>
        </contrib>
      </contrib-group>
      <aff id="I1"><sup>1</sup>Audiological Research Centre, &#xD6;rebro University Hospital, 701 85 &#xD6;rebro, Sweden</aff>
      <aff id="I2"><sup>2</sup>School of Health and Medical Sciences, &#xD6;rebro University, Fakultetsgatan 1, 70281 &#xD6;rebro, Sweden</aff>
      <aff id="I3"><sup>3</sup>Campus Alfred Nobel, &#xD6;rebro University, Karlav&#xE4;gen 16, 69141 Karlskoga, Sweden</aff>
      <aff id="I4"><sup>4</sup>Department of Ophthalmology, &#xD6;rebro University Hospital, 701 85 &#xD6;rebro, Sweden</aff>
      <author-notes>
        <corresp id="cor1">*Parivash Ranjbar: <email>parivash.ranjbar@orebroll.se</email></corresp>
        <fn fn-type="other">
          <p>Academic Editors: A. Deveze and M. Stankovic</p>
        </fn>
      </author-notes>
      <pub-date pub-type="collection">
        <year>2013</year>
      </pub-date>
      <pub-date pub-type="epub">
        <day>19</day>
        <month>6</month>
        <year>2013</year>
      </pub-date>
      <volume>2013</volume>
      <elocation-id>206734</elocation-id>
      <history>
        <date date-type="received">
          <day>29</day>
          <month>3</month>
          <year>2013</year>
        </date>
        <date date-type="accepted">
          <day>7</day>
          <month>5</month>
          <year>2013</year>
        </date>
      </history>
      <permissions>
        <copyright-statement>Copyright &#xA9; 2013 P. Ranjbar and I. Stenstr&#xF6;m.</copyright-statement>
        <copyright-year>2013</copyright-year>
        <license license-type="open-access">
          <license-p>This is an open access article distributed under the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited.</license-p>
        </license>
      </permissions>
      <abstract>
        <p>Monitor is a portable vibrotactile aid to improve the ability of people with severe hearing impairment or deafblindness to detect, identify, and recognize the direction of sound-producing events.
It transforms and adapts sounds to the frequency sensitivity range of the skin. The aid was evaluated in the field. Four females (44&#x2013;54 years) with Usher Syndrome I (three with tunnel vision and one
with only light perception) tested the aid at home and in traffic in three different field studies: without Monitor, with Monitor with an omnidirectional microphone, and with Monitor with a directional microphone.
The tests were video-documented, and the two field studies with Monitor were initiated after five weeks of training. The detection scores with omnidirectional and directional microphones were 100% for
three participants and above 57% for one, both in their home and traffic environments. In the home environment the identification scores with the omnidirectional microphone were 70%&#x2013;97%
and 58%&#x2013;95% with the directional microphone. The corresponding values in traffic were 29%&#x2013;100% and 65%&#x2013;100%, respectively. Their direction
perception was improved to some extent by both microphones. Monitor improved the ability of people with deafblindness to detect, identify, and recognize the direction of events producing sounds.</p>
      </abstract>
    </article-meta>
  </front>
  <body>
    <sec id="sec1">
      <title>1. Introduction</title>
      <p>Monitor is a device developed to give people with severe hearing impairment (HI) or deafblindness (DB) access to more information about events in their surroundings. The aid, Monitor, uses the vibratory sense and is programmed to handle environmental sounds in contrast to other vibratory aids designed for speech signals [<xref ref-type="bibr" rid="B20">1</xref>, <xref ref-type="bibr" rid="B18">2</xref>]. It detects sounds from events picked up by a microphone, adapts the sound to the frequency sensitivity range of the skin using algorithms developed based on modulating, transposition, or filtering principles, and translates the signal as vibrations. The person sensing the vibrations can detect and identify the character and direction of a sound source. </p>
      <p>The previous nonportable version of Monitor was evaluated by people with normal hearing, profound deafness, and with blindfolded deaf people in various laboratory and field studies [<xref ref-type="bibr" rid="B10">3</xref>&#x2013;<xref ref-type="bibr" rid="B13">7</xref>]. The results showed that Monitor consistently improved the ability of blindfolded deaf people to detect, identify, and recognize the direction of ongoing events producing sounds at home and in traffic. Four different algorithms, based on modulating and transposing principles, were found to be good candidates to be implemented in a portable vibrotactile aid for persons with DB. In this current study, Monitor's design was made portable by implementing a specific algorithm (one of the four selected in the laboratory) in a cell phone for a specific person with DB. The three microphones in a headband are also reduced to one microphone with two settings, omnidirectional and directional. After a period of training, the portable Monitor will be evaluated in this current study in a realistic environment by people with DB.</p>
      <p>There are about 400 people born deafblind in Sweden. About 1600 (&lt;65 years) became deafblind due to more than 30 progressive hereditary diseases affecting vision and hearing. The largest group, about 300 people, have Usher's Syndrome type I (US I). The largest group of DB, estimated at about 30,000&#x2013;40,000 people, is those over age 65 with a serious combination of vision and hearing impairments [<xref ref-type="bibr" rid="B3">8</xref>]. In Sweden and the northern countries, the definition of DB is based on the individual's special needs and not the degrees of vision and hearing impairment. The Nordic definition of DB is as follows.<disp-quote><p>&#x201C;Deafblindness is a Distinct Disability: Deafblindness is a combined vision and hearing disability. It limits activities of a person and restricts full participation in society to such a degree that society is required to facilitate specific services, environmental alterations and/or technology [<xref ref-type="bibr" rid="B2">9</xref>].&#x201D; </p></disp-quote></p>
      <p>People with DB belong to the category of persons with severe disabilities. The functional difficulties associated with DB are as follows.</p>
      <p>Communication and social interaction: the ability to communicate and exchange information with people in their surroundings.</p>
      <p>Mobility: the ability to move and orient themselves in their surroundings.</p>
      <p>Management of activities of daily living: the ability to independently perform daily activities.</p>
      <p>Environmental perception: the ability to perceive ongoing events in the environment and thereby implement appropriate forward planning and control (adapt to and influence).</p>
      <p>People with DB do not know what is going on around them. They want to know about ongoing events, such as a fire alarm, and objects below their visual field, for example, children, dog, or baby carriage, so that they do not stumble, or if they are in the path of a vehicle.</p>
      <p>Usually they have to use different senses, alone or combined, in different situations. They use information from vibrations, smell, taste, draught (air current), and temperature differences to detect events. Vision and touch are mainly used to identify the events [<xref ref-type="bibr" rid="B1">10</xref>].</p>
      <p>Hearing aids (HAS) and vibrotactile aids, for example, Tactaid II and Minivib, convey some environmental information, but these aids are programmed for speech and not for environmental sounds that often have a different frequency range [<xref ref-type="bibr" rid="B21">11</xref>&#x2013;<xref ref-type="bibr" rid="B15">13</xref>].</p>
      <p>The lack of information about ongoing events makes it difficult for people with DB to comprehend events in their surroundings, creating difficulties in forward planning and control; consequently, they sometimes feel frightened and stressed. People with DB are dependent on others, usually family members or interpreters, for information [<xref ref-type="bibr" rid="B1">10</xref>, <xref ref-type="bibr" rid="B16">14</xref>]. The present study with Monitor will deal with environmental perception problems. We will specially focus on &#x201C;getting to know,&#x201D; social interaction, safety, and mobility.</p>
      <p>In this study Monitor will be evaluated by people with DB using their specific algorithm in a natural environment after a period of training. We decided to evaluate the benefit of Monitor with a group of people of similar age, with a diagnosis leading to DB, who are well educated, able to collaborate and communicate, and who would benefit from a Monitor in their daily lives.</p>
      <p>The general purpose of this project is the evaluation of a portable version of Monitor in a field study. This was done by determining the following:<list list-type="order"><list-item><p>the ability to detect and identify ongoing events in their home and traffic environments without Monitor (observation with video recordings);</p></list-item><list-item><p>the ability to detect and identify ongoing events in their home and traffic environments with Monitor using an omnidirectional microphone (observation with video recordings);</p></list-item><list-item><p>the ability to detect, identify, and recognize the direction of ongoing events in their home and traffic environments with Monitor using a directional microphone (observation with video recordings).</p></list-item></list></p>
    </sec>
    <sec id="sec2">
      <title>2. Methods</title>
      <p>We used three field studies with the general aim of evaluating the Monitor.</p>
      <sec id="sec2.1">
        <title>2.1. Participants</title>
        <p>People with US I were chosen as participants (Ps) in the study. They are born deaf, have balance problems, have visual adaptation difficulties when they change from bright to dark environments and vice versa due to progressive retinitis pigmentosa (RP), with tunnel vision over time, and have night blindness [<xref ref-type="bibr" rid="B9">15</xref>]. Four females between the ages of 44 and 54 years (see <xref ref-type="table" rid="tab1">Table 1</xref> for anamnestic information) participated in the tests. Written information about the study, procedure, criteria of Ps, and contact information for the test leader was sent to those who met the requirements to determine their interest (Regional Ethics Committee in Uppsala, Sweden, Reg. No. 2006&#x2009;:&#x2009;216, revised 2011). Information about Ps' hearings and visions was collected after the test leader received the Ps' consents. The number of Ps is too small for statistical analysis but sufficient for a case study. They are all females (no male with US I reported interest). Three of them have children. They were educated at a high school for people with deafness. The Ps were paid for their participation in the study, which included daily reports of their experiences with Monitor, and performed field tests in their homes. All four Ps communicated by email, SMS, and sign language.</p>
      </sec>
      <sec id="sec2.2">
        <title>2.2. Equipment</title>
        <sec id="sec2.2.1">
          <title>2.2.1. Monitor</title>
          <p>The vibrotactile aid used in this project, Monitor, consists of a cell phone (HTC based on Android) containing an application, an external microphone, an amplifier (Wowpotas), and a vibrator (see <xref ref-type="fig" rid="fig1">Figure 1</xref>). The microphone and the vibrator are connected to the cell phone via the headset (input channel).</p>
        </sec>
        <sec id="sec2.2.2">
          <title>2.2.2. Application</title>
          <p>The application loaded in the cell phone, Monitor, was programmed with one of the four algorithms (see <xref ref-type="table" rid="tab2">Table 2</xref>) that showed the highest identification for the specific P in a laboratory setting. The four algorithms (AM, AMMC, TR, and TRHA), which had shown optimal results in previous studies, were used to adapt the sound from events to the sensitivity range of the skin [<xref ref-type="bibr" rid="B10">3</xref>&#x2013;<xref ref-type="bibr" rid="B11">6</xref>]. The four algorithms, with a short description of each, are shown in <xref ref-type="table" rid="tab2">Table 2</xref>.</p>
          <p>The algorithm AM transposed the temporal information in the input signal in the frequency range 0&#x2013;5500&#x2009;Hz using a single carrier wave, 250&#x2009;Hz.</p>
          <p>The AMMC algorithm transposed the temporal information in the input signal in the frequency range of 0&#x2013;5500&#x2009;Hz, using six carrier waves.</p>
          <p>The TR algorithm, transposed the temporal and spectral information in the range of 0&#x2013;5500&#x2009;Hz to the range of 0&#x2013;290&#x2009;Hz. This algorithm was not used in the field study because it did not show the highest identification score in any P (Ranjbar, in preparation). </p>
          <p>The fourth algorithm was TRHA, which transferred the spectral information to the frequency range of 50&#x2013;470&#x2009;Hz by selecting the 10 frequency components with the highest energy every 100 msec in the range of 0&#x2013;5500&#x2009;Hz.</p>
        </sec>
        <sec id="sec2.2.3">
          <title>2.2.3. Microphone</title>
          <p>The microphone (Phonac MM8) has two settings, omnidirectional and directional. <xref ref-type="fig" rid="fig2"> Figure 2</xref> shows the sensitivity of the microphone in the two different positions as a function of angle.</p>
          <p>The omnidirectional microphone is equally sensitive to sounds from all angles (see <xref ref-type="fig" rid="fig2">Figure 2(a)</xref>). In the directional position, it picks up sound from zero degrees azimuth without affecting its level while attenuating the sounds from the sides (see <xref ref-type="fig" rid="fig2">Figure 2(b)</xref>).</p>
          <p>The microphone was in its omnidirectional setting during training and testing in the first period of the field study and in the directional position in the second period of the field study (see Procedure).</p>
        </sec>
        <sec id="sec2.2.4">
          <title>2.2.4. Vibrator</title>
          <p>The vibrator used by the Ps was a C2-Tactor, and has been used in previous studies [<xref ref-type="bibr" rid="B10">3</xref>, <xref ref-type="bibr" rid="B14">5</xref>&#x2013;<xref ref-type="bibr" rid="B13">7</xref>]. The vibrator has a frequency range between 10 and 350&#x2009;Hz with a peak at 80&#x2009;Hz (see <xref ref-type="fig" rid="fig3">Figure 3</xref>).</p>
        </sec>
        <sec id="sec2.2.5">
          <title>2.2.5. Test Stimuli</title>
          <p>In the present study, important events producing sounds were selected and presented a different number of times for each P in three different field tests (see further <italic>Procedure</italic>). The events (see <xref ref-type="table" rid="tab3">Table 3</xref>) were selected in previous studies [<xref ref-type="bibr" rid="B1">10</xref>] by people with DB and people with normal hearing [<xref ref-type="bibr" rid="B12">4</xref>] and in this current study by the Ps as important events to be informed about. The first eight sounds (1&#x2013;8) in the home environment were the same when testing all Ps in the three different field tests. The remaining sounds (9&#x2013;15) could be different for the four different Ps or for the same P in the three different field tests. The variations of test stimuli in the home environment were dependent on equipment that the test leaders had access to or the Ps' habits, for example, one is a coffee drinker and uses a coffee maker, while another is a tea drinker and uses an electric kettle.</p>
        </sec>
      </sec>
      <sec id="sec2.3">
        <title>2.3. Procedure</title>
        <p>The investigation consisted of three field studies. The first field study included one part, Field test, while the two other field studies included two parts, <italic>training</italic> and field test (documented by video for analysis).<list list-type="order"><list-item><p>Field test without Monitor (noM),</p></list-item><list-item><p>field training and test with Monitor with omnidirectional microphone (MO),</p></list-item><list-item><p>field training and test with Monitor with directional microphone (MD).</p></list-item></list></p>
        <p>When performing the field tests, the Ps' own technical aids, for example, hearing aid or doorbell detector, were removed.</p>
        <sec id="sec2.3.1">
          <title>2.3.1. Field Test without Monitor (noM)</title>
          <p>This step included one part: test. The ability of the Ps to perceive their environments was tested at each P's home and traffic environments. Four test leaders (TL1, TL2, TL3, and TL4) were involved in the tests. TL1, TL2, and TL3 initiated different events producing sounds (see <xref ref-type="table" rid="tab3">Table 3</xref>). TL1 also observed and documented the test, and TL4 continuously filmed the test. The events were initiated in random order one or more times using the Ps' own objects. The events were performed in a process, for example, when doing the event &#x201C;popping popcorn in a microwave,&#x201D; the door the microwave was opened of the microwave timer was activated and so on. Along with the planned events shown in <xref ref-type="table" rid="tab3">Table 3</xref>, there were also some unplanned events, for example, a person passing.</p>
          <p>
<italic>Home.</italic> The test was performed in the Ps' own homes, where they were sitting in a relaxed manner with their backs to the test environment to decrease the visual clues of the events. The Ps were encouraged to use all their abilities to detect and identify the ongoing events. The order of the events was determined by a list that was known only to the test leaders. When the Ps detected the event, they signaled their detection by raising their hand and then continued to try to identify the event. The Ps were then also allowed to move and look around and find the sound source and identify the event. During the test, there was a person (an assistant or a relative to the P) with normal vision available who could help the test leaders find the objects that the test leaders needed to produce sound with (e.g., vacuum cleaner). The same person was also allowed to give feedback about the event that the P had detected and identified.</p>
          <p>
<italic>Traffic.</italic> In the traffic test, the P was walking on a well-known path (e.g., the street between home and her children's school) and using her senses to detect and identify the events (see <xref ref-type="table" rid="tab3">Table 3</xref>). When the P detected the event, she raised up her hand to make clear to the test leaders that she had detected something. After detection, she signaled the identity of the event. In the case of P1, who is blind, her assistant was holding her arm and accompanied her without giving any signals or feedback, necessary to learning [<xref ref-type="bibr" rid="B7">16</xref>]. The Ps were allowed to turn and visually search for the detected events in order to identify them.</p>
        </sec>
        <sec id="sec2.3.2">
          <title>2.3.2. Field Training and Test with Monitor with Omnidirectional Microphone (MO)</title>
          <p>This step included two parts: training and test. The microphone detected sounds from all directions (see <xref ref-type="fig" rid="fig2">Figure 2(a)</xref>).</p>
          <p>
<italic>Training, Daily Use/Report for Five Weeks.</italic> The Ps were instructed to practice as much as they could (waking time) in their residence areas (home, external activities/work, and traffic). The structure of the training was individually different. This made it easier for them to be trained for a longer time instead of the Ps going to the same place and getting trained for 2-3 hours a day. The Ps received written and signed information and instruction about how to handle the equipment and how to send a daily report to the test leader. The number of training days was extended if the P could not be trained, for example, if Monitor was damaged or the P had to take a break. The subject reported daily under the following headings:</p>
          <p>day, number of training hours, occasions when Monitor made benefit, occasions when Monitor caused problems, occasions when Monitor made no use, and other comments.</p>
          <p>
<italic>Test.</italic> After five weeks, the Ps were tested at home and in traffic in the same way, at the same place, and exposed to the same events (random order) as explained in field test <italic>noM. </italic>The difference was that the Ps also used the information from Monitor to detect and identify the events. The events were presented in a random order. Each event was generated as similarly as possible to the previous test when the Ps did not use Monitor. The test was documented with video recordings.</p>
        </sec>
        <sec id="sec2.3.3">
          <title>2.3.3. Field Training and Test with Monitor with Directional Microphone (MD)</title>
          <p>This step also included two parts: training and test. The microphone was in its directional position so that the detected sounds from behind were attenuated (see <xref ref-type="fig" rid="fig2">Figure 2(b)</xref>).</p>
          <p>
<italic>Training, Daily Use/Report in Five Weeks.</italic> The Ps were trained for another five-week period using Monitor with directional microphone (MD). The P was encouraged to also identify the direction of the events. The Ps continued to send a daily report to the test leader. The conditions were the same as in the earlier five-week training.</p>
          <p>
<italic>Test.</italic> The Ps were tested at home and in traffic in the same way as field test <italic>noM </italic>andfield test MO. The events were as identical as possible to the previous tests but with a different random order. The test was documented with video recordings.</p>
        </sec>
      </sec>
      <sec id="sec2.4">
        <title>2.4. Analysis Methods</title>
        <p>In this case study, each P was analyzed and reported separately. The video recordings from field tests noM, MO, and MD were evaluated. Critical parts were evaluated by two independent observers. A correct detection and identification resulted in one point, and an incorrect response resulted in zero points. A correct response means that Ps signed their detection of the events by raising their hand or they began to identify the event. A correct identification means that the Ps identified the event exactly (not partly) correct.</p>
      </sec>
    </sec>
    <sec id="sec3">
      <title>3. Results</title>
      <sec id="sec3.1">
        <title>3.1. Training, Daily Reports with MO/MD</title>
        <p>The Ps lived their normal lives and performed their daily activities. In the first training period, they used Monitor (MO) on average of 4&#x2013;10 hours daily, and in the second period of training, they used Monitor (MD) to average of 3&#x2013;11 hours daily. The number of training days and average training hours/day for each P for the two training periods are shown in <xref ref-type="table" rid="tab4">Table 4</xref>.</p>
        <p>P1, with the longest training period, was the first to begin the training at the end of November. She continued with the training even after completing her five training weeks while waiting for the test leader to organize a test opportunity, which took a long time due to unexpected delays including late equipment delivery and finding a common day convenient for all involved in the test. In the daily reports with Monitor, they described which sounds from events at home and outdoors they had sensed as vibrations (e.g., coffee maker and boiling water). They also reported sounds produced by themselves or others, for example, footsteps, breathing, eating, drinking, own laugh, and laughs from grandchildren. </p>
        <p>The Ps had an increased awareness of sounds they produced thereby helping them control their behavior as an important factor in their social life.</p>
        <p>The Ps could detect the people nearby, for example, when one of the Ps sensed speech and discovered that her husband was talking to the dog.</p>
        <p>All three Ps with remaining vision had used Monitor when watching TV and discovered such new things as the high volume of music during TV advertising. All four Ps reported music as a new and pleasant experience. </p>
        <p>The information from Monitor could help them have better social interaction and they could act/react, for example, P1 could calm her friend's arguing children. Her reaction surprised the children, who knew that P1 cannot see or hear. P1 also reported that she could sense vibrations when her friend was talking on a cell phone and so P1 kept silent to not disturb her.</p>
        <p>Monitor helped them to have better forward planning and save time. They reported better control over household machines, for example, one could turn off boiling water or empty the washing machine when she sensed the vibrations.</p>
        <p>Monitor could improve their safety by informing/warning them, for example, one of the Ps could take cover when noting the vibrations of an angry voice. Monitor could also inform them about the presence of cars so they could move away from the car's direction and feel safe.</p>
        <p>During the second training period, they had become more curious and explored new sounds, for example, they clapped their hands, they tried to talk (said yes, no, thanks, hello, or goodbye), laughed at sensing their own voices, or they sensed the thunder and rain. P1 reported that the vibrations were stronger when the directional microphone was directed toward the sound source. She could move around with an extended arm with the microphone on and scan the environment to identify the sound source. P3 reported every day that the Monitor was good and she was happy about the information it delivered. Her directional perception had been improved but she could not explain how. Once, she had felt vibrations from a car and for some reason looked at the back of the house where the car was and not in the front, even though cars can drive in the front as well.</p>
        <p>Monitor was often of benefit but it also had difficulties, for example, when it was very noisy (when travelling by car or train), the Monitor was vibrating all the time and it was difficult to distinguish between the noise from the train and the voices. A fully charged amplifier could function for a maximum of 13 hours.</p>
        <p>The Ps preferred the omnidirectional microphone because they could miss important sounds using the directional microphone. The vibrations of the directional microphone were too weak and difficult to sense when it was not directed toward the sound source and too strong when it was.</p>
        <p>They had problems with the cables, for example, one had to use extra tape to keep the Monitor in place and prevent it from dropping. In hot weather, their arms were sweaty. Some Ps thought that the cords were a problem, for example, when cooking food, they were afraid of burning them. Some other Ps felt that the cords were beneficial since they kept all pieces of Monitor in the same place.</p>
        <p>All Ps took some break periods due to technical problems or private reasons. No one reported any occasion when the Monitor was of no benefit.</p>
      </sec>
      <sec id="sec3.2">
        <title>3.2. Field Test, noM, MO, and MD</title>
        <sec id="sec3.2.1">
          <title>3.2.1. Home Environment</title>
          <p>The detection and identifications results of the three field tests at home for each P are shown in <xref ref-type="fig" rid="fig4">Figure 4</xref>.</p>
          <p>The numbers of presented test stimuli for each P varied and were as follows:<list list-type="simple"><list-item><label>&#x2009;</label><p>field test noM, P1 (28), P2 (16), P3 (18), and P4&#x2009;&#x2009;(17);</p></list-item><list-item><label>&#x2009;</label><p>field test with MO, P1 (17), P2 (20), P3 (19), and P4&#x2009;&#x2009;(25);</p></list-item><list-item><label>&#x2009;</label><p>field test with MD, P1 (31), P2 (15), P3 (22), and P4&#x2009;&#x2009;(18).</p></list-item></list></p>
          <p>
<xref ref-type="fig" rid="fig4">Figure 4</xref> shows that both detection (18%&#x2013;33%) and identification scores (11%&#x2013;19%) are lowest when Ps were tested noM. When using MO and MD, the detection scores increased to 100% for P2, P3, and P4 and for P1 to 94% and 97%, respectively.</p>
          <p>When using MO, the increments of identification scores were as follows: P1 (60), P2 (81), P3 (78), and P4 (54) percentage units. The corresponding results for test with MD were P1 (47), P2 (48), P3 (78), and P4 (76) percentage units.</p>
          <p>
<italic>Field Test noM, Details.</italic> The Ps born deaf could all detect the event when other people (test leaders, their assistant, or the interpreter) were passing behind by feeling their air current or smell. The event &#x201C;door opening and closing&#x201D; was detected by all of the Ps when the door was closed hard and the Ps felt the cold wind coming in via the door. All the Ps could detect and identify the event &#x201C;vacuum cleaner,&#x201D; which was detected when the test leader was cleaning under the chair or touching the chair that the Ps were sitting on (vibratory sense). They could also detect the events &#x201C;popping popcorn in a microwave oven&#x201D; and &#x201C;coffee maker&#x201D; after the event was finished and they could smell. They did not detect the remaining test events. </p>
          <p>
<italic>Field Test MO, Details.</italic> As shown in <xref ref-type="fig" rid="fig4">Figure 4</xref>, the detection score of each P was 100% except for P1, whose detection score was 94%. She could not detect the event &#x201C;water flushing&#x201D; which occurred in the washroom while she was in the hall (two meters away).</p>
          <p>P1 could identify 70% of the events correctly. She identified the event &#x201C;telephone signaling&#x201D; as persons laughing which was similar to the signal that was a musical melody. She identified the event &#x201C;door opening and closing&#x201D; as &#x201C;a person sneezing&#x201D; which is as short and strong as the event &#x201C;door opening and closing.&#x201D; </p>
          <p>P2 could correctly identify 80% of the events but confused the event &#x201C;a person coughing&#x201D; with the event &#x201C;dropping keys&#x201D; which are both short and strong. She could not identify the events &#x201C;a person talking,&#x201D; and &#x201C;dropping keys,&#x201D;</p>
          <p>P3 could identify 94% of the events correctly. She confused the event &#x201C;door opening and closing&#x201D; with the event &#x201C;a person sneezing.&#x201D;</p>
          <p>P4 correctly identified 72% of the events and confused the event &#x201C;a person coughing&#x201D; with the events &#x201C;doorbell,&#x201D; &#x201C;water dropping,&#x201D; or &#x201C;something strong.&#x201D; She also confused the event &#x201C;closing the door&#x201D; with &#x201C;a person coughing.&#x201D; She could not identify the event &#x201C;a person talking.&#x201D; </p>
          <p>
<italic>Field Test MD, Details.</italic> The detection score of all Ps was 100% except P1 who could not detect &#x201C;water flushing&#x201D; in the washroom while she was in the hall two meters away. Ps could identify faster and were more detailed compared to previous tests, for example, P4 could identify the signal produced by the buttons of the timer when conducting the event &#x201C;popping popcorn in a microwave,&#x201D; where time was set to 3 minutes, by pushing the buttons. P2 could identify the telephone on the first ring but waited until the second ring to be sure. P1 was often hesitant and changed her mind several times before settling on her final response; therefore, she was tested with more events than the other three Ps. She could identify the direction of a sound source by stretching her arm with the microphone and scanning the area. P3 could identify events without using her vision. When the event &#x201C;water boiling&#x201D; was conducted, she responded as &#x201C;breeze&#x201D; but changed her response and explained the difference between the two events: &#x201C;water boiling&#x201D; starts out faint and becomes stronger over time, but the &#x201C;breeze&#x201D; has the same intensity all the time. The only event P4 could not identify was &#x201C;toilet flushing once&#x201D; which she explained by the fact that she was expecting that it would flush two times.</p>
        </sec>
        <sec id="sec3.2.2">
          <title>3.2.2. Traffic Environment</title>
          <p>The detection and identification results of the three field tests in traffic for each P are shown in <xref ref-type="fig" rid="fig5">Figure 5</xref>. The numbers of test stimuli for each P varied and were as follows.</p>
          <p>Field test noM, P1 (12), P2 (18), P3 (16), and P4 (15); field test MO, P1 (7), P2 (10), P3 (20), and P4 (25); field test MD, P1 (26), P2 (11), P3 (16), and P4 (20).</p>
          <p>
<xref ref-type="fig" rid="fig5">Figure 5</xref> shows that when testing noM, the detection and identification scores varied from 0% for P1 to 100% for P4, who was able to detect the events when she was five meters or more away, depending on the object&#x2014;person, car, or bicycle&#x2014;in front of her. It is to be noted that in the noM situation the Ps only detected the events coming from behind when the object had passed them. They could not act or react other than to be startled. There was no forewarning. Monitor helped to detect and identify the events when approaching from behind and thereby increased their safety. When using MO and MD, the detection scores were 100% for P2, P3, and P4 and 57% for P1. The corresponding identification scores were P1 (29%), P2 (60%), P3 (25%), and P4 (100%).</p>
          <p>When using MO, the increment of identification scores was as follows: P1 (29), P2 (38), P3 (75), and P4 (0) percentage units. The corresponding results for test with MD were P1 (65), P2 (31), P3 (63), and P4 (0) percentage units. </p>
          <p>
<italic>Field Test noM, Details.</italic> P2, P3, and P4 could detect and identify the person, car, and bicycle coming towards them using their severely restricted visual field (tunnel vision).</p>
          <p>When the person came from behind or was close to their side, the Ps said they were not aware of the event because they could not hear. When the person was close to their side, the Ps said they were not aware because they could not hear and not see due to their tunnel vision. When the person came in front of Ps, he/she was suddenly detected and identified. They are often startled because the person was unexpected. The three situations above are illustrated in <xref ref-type="fig" rid="fig6">Figure 6</xref>.</p>
          <p>The reaction was the same for the events &#x201C;car&#x201D; and &#x201C;bicycle.&#x201D; The Ps could detect and identify the car at about 50 meters or more in front and the bicycle at about five meters or more depending on the vehicle's size. </p>
          <p>P3 could not detect the car the first time it was in front of her because its contrast was poor. P1, who has only light perception, could not detect or identify any event. The Ps did not react to any sound such as signal, talk, or running steps.</p>
          <p>
<italic>Field Test MO.</italic> The detection scores of P2, P3, and P4 were 100%, while P1 scored 57%. P2, P3, and P4 with residual vision could detect events that were several meters outside their visual field. These Ps reported that they could immediately identify the event by the vibrations, but they also turned their eyes towards the sound source to see if they had identified it correctly. They also wanted to show the sound source to the test leaders who were documenting the test on video. </p>
          <p>
<italic>Field Test MD.</italic> The Ps could detect all the events (100%) except P1, whose detection score was 77%. P1 often forgot to signal her detection by raising her hand.</p>
          <p>The identification scores were as follows: P1 (65%), P2 (90%), P3 (82%), and P4 (100%).</p>
          <p>All three Ps (P2, P3, and P4) with residual vision used their vision to identify and localize the events, but in many cases they could recognize the direction of the event without using their visual sense. P2, P3, and P4 could identify the events very quickly.</p>
          <p>P2 could identify the events even without using her vision. When she was tested on a sunny day, she was blinded by the sun and had to shade her eyes with her hand, despite sunglasses. Therefore, she could not use her fingers to better sense the vibrations and missed the faint vibrations produced by, for example, the bicycle.</p>
          <p>P1 was tested in a path that was noisier than the path of the other three Ps. P1 was sometimes confused and forgot to signal her detection. In some cases, she did not signal but began to identify directly. In those cases when she did not signal her detection, the test leaders were not sure if she did not detect or forgot to signal. She even identified some unplanned events, such as a lawnmower. She could recognize the direction of the lawnmower exactly, which was at least 100 meters from her. She was critical of the directional microphone because its signal becomes low and undetectable when her hand was hanging and the microphone was directed at the ground.</p>
          <p>P3 was certain in her identification and could even distinguish between the two different signals produced by the two different bicycles. She could point out the direction of the events with her hand without turning her face to the direction of the sound source.</p>
        </sec>
      </sec>
    </sec>
    <sec id="sec4">
      <title>4. Discussion</title>
      <p>The focus of the discussion will be on some limitations and implications of the study.</p>
      <sec id="sec4.1">
        <title>4.1. Aspects of Methods</title>
        <p>A case study can be designed to have a large variability regarding properties of the Ps: age, cognitive ability, vibratory sensitivity, and training. In the present study, we tried to have a homogenous group.</p>
        <p>The participants were selected from the largest diagnostic category of DB, US I. Hearing loss in these individuals is innate, that is, from birth, and thus, they have the same conditions in terms of hearing ability. Participants with other vision and hearing problems would have different conditions and performance. The P can have low sensitivity to vibrations or they can be unmotivated to train, which are important factors affecting the results. It is likely that the Ps were highly motivated since the average time (hours/day) they used the Monitor was high, about 7 hours/day in the first five weeks of training, which increased to 8 hours/day in the second five weeks, since the fully charged amplifier could work a maximum of 13 hours. They appeared very honest and reported with high credibility, for example, P3 reported that she had used the Monitor but forgot to concentrate on the vibrations because she was too busy with household tasks. They sometimes reported that they had a migraine or headache, and therefore, they could not train. In spite of the relative homogeneity of the participating cases, the individual situations varied markedly as well as the use and benefit of the Monitor. It is clear that an individual analysis has to be made regarding the needs and use of this aid as well as most other aids for people with severe sensory impairments [<xref ref-type="bibr" rid="B4">17</xref>&#x2013;<xref ref-type="bibr" rid="B8">19</xref>].</p>
        <p>The test situations were chosen for having a high validity in the daily life situation of each individual. A complementary study with laboratory tests is in preparation (Ranjbar, in preparation). Few other studies have presented a similar design with video recordings in natural environments and we regard this technique as suitable for analyzing the effect of technical aids in cases of complex impairments [<xref ref-type="bibr" rid="B6">20</xref>]. The algorithms used in the study are similar to those used in earlier studies showing equally good results [<xref ref-type="bibr" rid="B10">3</xref>&#x2013;<xref ref-type="bibr" rid="B13">7</xref>]. They were simplified to some extent and a further simplification may be needed; it is important to not overload the capacity of the cutaneous senses. On the other hand, systematic use of receptors other than the vibratory (touch and pressure) can increase the transmission and central treatment capacity of the haptic system. The frequency bandwidth of the vibrator we used does not cover the whole range of the vibratory system [<xref ref-type="bibr" rid="B19">21</xref>]. A compromise between physical clumsiness and bandwidth is required.</p>
        <p>The tool, Monitor, had the same structure for all Ps; the vibrator could be placed where the P is sensitive to vibrations and feels comfortable. As with all other technical equipment, some technical problems occurred when using the Monitor, for example, the cables could be worn or loose, the amplifier had technical failures, the P had unconsciously turned off the volume, and a fully charged amplifier can only function a maximum of 13 hours, which needs to be improved and extended. The cords can be shortened, or a wireless version can be designed in future studies.</p>
        <p>Monitor was used and tested in the Ps' residences and in traffic, which was different for each P but made it easier for the P to be trained many hours/day in a real environment without any uncomfortable and awkward changes in their daily life. Also, the variations in the events were greater, and the situations were more natural than if they were trained at the laboratory using the sounds presented via a loudspeaker. This uncontrolled environment could cause some difficulties when testing. For example, when testing P1, the test leaders made coffee using her coffee maker, but the P had not been trained with a coffee machine since she was a tea drinker. On another test occasion, the test leader boiled water in a pot, but the P normally used an electric kettle, which has a different sound. The conditions of the test cases were different for each P, but almost the same for same Ps at different test times (test noM, test MO, and test MD). Even in the traffic environment, the events producing sounds were different on different test occasions, for example, the car was not the same in all tests.</p>
        <p>Test sounds were selected by people with DB as representative of the sounds at home and traffic environments in a previous study [<xref ref-type="bibr" rid="B12">4</xref>, <xref ref-type="bibr" rid="B1">10</xref>]. They were almost the same for all Ps. The events were easy to perform without disturbing the subjects' privacy too much. The sounds can also vary depending on the source of the sound. For example, a vacuum cleaner or coffee maker can make a sound different depending on the specific device producing the sound. Therefore, tests were done in the P's own residence using the appliances that the P usually uses.</p>
      </sec>
      <sec id="sec4.2">
        <title>4.2. Aspects of Results</title>
        <sec id="sec4.2.1">
          <title>4.2.1. Training, Daily Reports</title>
          <p>P1, P2, and P4 had been trained for fewer days during the second period field study with MD than the first period, which can be interpreted as reduced interest. The differences can be explained by difficulties when organizing a common test day suited for all people involved when conducting the tests in the first training period. On the other hand, in the second period, they were trained more hours in the same day, indicating increased interest and getting used to handling the Monitor which, according to Ps, became easier with training.</p>
          <p>In the beginning, the Ps sensed and reported what they had done in their daily lives; later, they became more curious to explore new sounds like thunder or their own voices. P1 was trained longer than the other three, but still had lower results than them. This can be explained by the fact that she had no useful visual acuity and thereby could not get as much feedback as the others. Her assistants were often people with profound deafness, who could not always inform her about events they had not heard themselves. The volume of P1's Monitor was often too high, which can reduce the dynamic of the vibrator and thereby give a flat pattern to the vibrations. The reduced dynamic could have a more negative effect, especially in noisy environments such as in traffic where P1 was tested. An effective and standard training program similar to those used for cochlear implant and other aids should be designed and tested in future studies [<xref ref-type="bibr" rid="B4">17</xref>].</p>
        </sec>
        <sec id="sec4.2.2">
          <title>4.2.2. Field Tests</title>
          <p>Both detection and identification scores were high. The detection scores of Ps with residual vision were 100% both at home and in traffic with Monitor with MO/MD. The Ps had also high identification scores (&gt;67%) with Monitor. They confused events with similar signal patterns. The results in traffic were affected by difficult weather conditions, for example, it was windy and the vibrator vibrated all the time, or it was &#x2212;15 degrees, and therefore, the P could not use her fingers to feel the vibrations better. The Ps also had balance problems and had to focus on walking. Monitor was evaluated in a realistic environment and should become more robust for different conditions. An algorithm with noise reduction and automatic volume adaptation should be developed.</p>
          <p>The directional microphone delivered directional information but could also miss some important sounds. A new version with a combined directional and omnidirectional microphone should be designed.</p>
          <p>Monitor was able to inform the Ps about ongoing events producing sounds, resulting in increased forward planning, improved feeling of safety, increased social interaction, and increased mobility. Monitor enhanced the benefit they got from their remaining vision, especially in traffic. As a disadvantage, they mentioned that Monitor cannot detect events that do not produce sounds, which is a problem. The residential area of DB people is very often quiet, for example, people nearby do not talk loudly because the Ps cannot hear. The environment should be adapted as for people with normal hearing, for example, the people nearby should use more of their voices to get the deafblind person's attention instead of just tapping them.</p>
          <p>The Ps had improved directional recognition. They could even detect events that were behind them and out of their field of vision, especially in traffic. They could frequently and more easily use their vision to recognize the direction of events or by scanning the environment to feel where the sound was stronger, indicating a directional correction toward the sound source. Using the directional microphone, they could also recognize the direction of simultaneous sounds from different sources. In some cases, they could miss important things because the microphone was not directed at them. An alternative construction of the directional microphone is of interest in future studies.</p>
          <p>Monitor improved their social interaction. By sensing talk from other people, they could keep quiet until it was their turn to talk, or simply not bother them. They could control their own body sounds, for example, when eating, talking loudly, or signing to a deaf person.</p>
          <p>Monitor improved their mobility by informing them about approaching cars, motorcycles, running persons and so on so they could move out of the way. If they dropped something they could detect and look for it and not stumble on it.</p>
          <p>They had reduced stress because they could get information about events in advance, resulting in improved forward planning.</p>
          <p>They thought that using Monitor was interesting, and they curiously explored new sounds with high motivation even after their five-week training, while waiting to be tested.</p>
        </sec>
      </sec>
    </sec>
    <sec id="sec5">
      <title>5. Conclusions and Further Developments</title>
      <p>Monitor, the vibrotactile aid for environmental perception, could improve the ability of people with DB to detect, identify, and recognize the direction of ongoing events producing sounds. The results showed that Monitor could improve their mobility, social interaction, forward planning, and feeling of safety and decrease their feelings of stress.</p>
      <p>A further development should be focused on the following.</p>
      <p>A new version with a combined directional and omnidirectional microphone should be designed to deliver both the identity of the sound without attenuating and the directional information.</p>
      <p>Training programs comparable to cochlear implant training programs should be developed to increase the effectiveness of environmental sound identification.</p>
      <p>The design of Monitor should be improved by making it wireless.</p>
    </sec>
  </body>
  <back>
    <ack>
      <title>Ethical Approval</title>
      <p>The study was approved by the Regional Ethics Committee in Uppsala, Sweden, Registration number 2006&#x2009;:&#x2009;216 (revised 2011).</p>
    </ack>
    <ack>
      <title>Acknowledgments</title>
      <p>The project has been supported by NovaMedTech, a European development foundation, Promobilia Foundation, Stingerfonden, Audiological Research Centre at &#xD6;rebro, and Campus Alfred Nobel at Karlskoga. </p>
    </ack>
    <ref-list>
      <ref id="B20">
        <label>1</label>
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Traunm&#xFC;ller</surname>
              <given-names>H</given-names>
            </name>
          </person-group>
          <source>
            <italic>The Sentiphone: A Tactile Communication Aid for Deaf</italic>
          </source>
          <year>1977</year>
          <publisher-name>Stockholm Department of Speech Communication and Music Acoustics</publisher-name>
        </element-citation>
      </ref>
      <ref id="B18">
        <label>2</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Spens</surname>
              <given-names>KE</given-names>
            </name>
            <name>
              <surname>Huss</surname>
              <given-names>C</given-names>
            </name>
            <name>
              <surname>Dahlqvist</surname>
              <given-names>M</given-names>
            </name>
            <name>
              <surname>Agelfors</surname>
              <given-names>E</given-names>
            </name>
          </person-group>
          <article-title>A hand held two-channel vibro-tactile speech communication aid for the deaf: characteristics and results</article-title>
          <source>
            <italic>Scandinavian Audiology Supplementum</italic>
          </source>
          <year>1997</year>
          <volume>26</volume>
          <issue>47</issue>
          <fpage>7</fpage>
          <lpage>13</lpage>
          <pub-id pub-id-type="other">2-s2.0-0030828522</pub-id>
          <pub-id pub-id-type="pmid">9428037</pub-id>
        </element-citation>
      </ref>
      <ref id="B10">
        <label>3</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Ranjbar</surname>
              <given-names>P</given-names>
            </name>
          </person-group>
          <article-title>Vibrotactile identification of signal-processed sounds from environmental events presented by a portable vibrator: a laboratory study</article-title>
          <source>
            <italic>Iranian Rehabilitation Journal</italic>
          </source>
          <year>2008</year>
          <volume>6</volume>
          <issue>7-8</issue>
          <fpage>24</fpage>
          <lpage>38</lpage>
        </element-citation>
      </ref>
      <ref id="B12">
        <label>4</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Ranjbar</surname>
              <given-names>P</given-names>
            </name>
            <name>
              <surname>Borg</surname>
              <given-names>E</given-names>
            </name>
            <name>
              <surname>Philipson</surname>
              <given-names>L</given-names>
            </name>
            <name>
              <surname>Stranneby</surname>
              <given-names>D</given-names>
            </name>
          </person-group>
          <article-title>Auditive identification of signal-processed environmental sounds: monitoring the environment</article-title>
          <source>
            <italic>International Journal of Audiology</italic>
          </source>
          <year>2008</year>
          <volume>47</volume>
          <issue>12</issue>
          <fpage>724</fpage>
          <lpage>736</lpage>
          <pub-id pub-id-type="other">2-s2.0-57649133960</pub-id>
          <pub-id pub-id-type="pmid">19085397</pub-id>
        </element-citation>
      </ref>
      <ref id="B14">
        <label>5</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Ranjbar</surname>
              <given-names>P</given-names>
            </name>
            <name>
              <surname>Anderz&#xE9;n-Carlsson</surname>
              <given-names>A</given-names>
            </name>
            <name>
              <surname>Neovius</surname>
              <given-names>L</given-names>
            </name>
            <name>
              <surname>Johansson</surname>
              <given-names>C</given-names>
            </name>
            <name>
              <surname>Borg</surname>
              <given-names>E</given-names>
            </name>
          </person-group>
          <article-title>Vibrotactile detection, identification and directional perception of signal-processed sounds from environmental events: a pilot field evaluation in five cases.</article-title>
          <source>
            <italic>Iranian Rehabilitation Journal</italic>
          </source>
          <year>2008</year>
          <volume>6</volume>
          <fpage>789</fpage>
          <lpage>8107</lpage>
        </element-citation>
      </ref>
      <ref id="B11">
        <label>6</label>
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Ranjbar</surname>
              <given-names>P</given-names>
            </name>
          </person-group>
          <source>
            <italic>Sensing the Environment: Development of Monitoring Aids for Persons with Profound Deafness or Deafblindness</italic>
          </source>
          <year>2009</year>
          <publisher-name>School of Science and Technology, &#xD6;rebro &#xD6;rebro Universitet</publisher-name>
        </element-citation>
      </ref>
      <ref id="B13">
        <label>7</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Ranjbar</surname>
              <given-names>P</given-names>
            </name>
            <name>
              <surname>Stranneby</surname>
              <given-names>D</given-names>
            </name>
            <name>
              <surname>Borg</surname>
              <given-names>E</given-names>
            </name>
          </person-group>
          <article-title>Vibrotactile identification of signal-processed sounds from environmental events</article-title>
          <source>
            <italic>Journal of Rehabilitation Research and Development</italic>
          </source>
          <year>2009</year>
          <volume>46</volume>
          <issue>8</issue>
          <fpage>1021</fpage>
          <lpage>1036</lpage>
          <pub-id pub-id-type="other">2-s2.0-75749141318</pub-id>
          <pub-id pub-id-type="pmid">20157859</pub-id>
        </element-citation>
      </ref>
      <ref id="B3">
        <label>8</label>
        <element-citation publication-type="other">
          <person-group person-group-type="author">
            <name>
              <surname>Edberg</surname>
              <given-names>P</given-names>
            </name>
            <name>
              <surname>Wahlquist</surname>
              <given-names>UP</given-names>
            </name>
            <name>
              <surname>Larsby</surname>
              <given-names>B</given-names>
            </name>
            <name>
              <surname>H&#xE4;llgren</surname>
              <given-names>M</given-names>
            </name>
          </person-group>
          <article-title>Serious combination of hearing- and vision impairment with persons, age of above 65</article-title>
          <comment>Link&#xF6;pings Universitet, Link&#xF6;ping, Sweden, 2008</comment>
        </element-citation>
      </ref>
      <ref id="B2">
        <label>9</label>
        <element-citation publication-type="other">
          <person-group person-group-type="author">
            <name>
              <surname>Deafblind</surname>
              <given-names>TAotS</given-names>
            </name>
          </person-group>
          <article-title>D&#xF6;vblindhet/definition, 2009</article-title>
          <comment>2007</comment>
        </element-citation>
      </ref>
      <ref id="B1">
        <label>10</label>
        <element-citation publication-type="confproc">
          <person-group person-group-type="author">
            <name>
              <surname>Borg</surname>
              <given-names>E</given-names>
            </name>
            <name>
              <surname>R&#xF6;nnberg</surname>
              <given-names>J</given-names>
            </name>
            <name>
              <surname>Neovius</surname>
              <given-names>L</given-names>
            </name>
            <name>
              <surname>M&#xF6;ller</surname>
              <given-names>K</given-names>
            </name>
          </person-group>
          <article-title>Monitoring environmental events: problems, strategies and sensory compensation</article-title>
          <conf-name>Proceedings of the ISAC'00 Conference</conf-name>
          <conf-date>2000</conf-date>
          <conf-loc>Exeter, UK</conf-loc>
        </element-citation>
      </ref>
      <ref id="B21">
        <label>11</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Traunm&#xFC;ller</surname>
              <given-names>H</given-names>
            </name>
          </person-group>
          <article-title>The sentiphone: a tactual speech communication aid</article-title>
          <source>
            <italic>Journal of Communication Disorders</italic>
          </source>
          <year>1980</year>
          <volume>13</volume>
          <issue>3</issue>
          <fpage>183</fpage>
          <lpage>193</lpage>
          <pub-id pub-id-type="other">2-s2.0-0019011433</pub-id>
          <pub-id pub-id-type="pmid">6445912</pub-id>
        </element-citation>
      </ref>
      <ref id="B17">
        <label>12</label>
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Spens</surname>
              <given-names>KE</given-names>
            </name>
          </person-group>
          <source>
            <italic>To &#x201C;Hear&#x201D; with the Skin</italic>
          </source>
          <year>1984</year>
          <publisher-name>Stockholm Department of Speech Communication and Music Acoustic, Kungliga Tekniska H&#xF6;gskolan</publisher-name>
        </element-citation>
      </ref>
      <ref id="B15">
        <label>13</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Reed</surname>
              <given-names>CM</given-names>
            </name>
            <name>
              <surname>Delhorne</surname>
              <given-names>LA</given-names>
            </name>
          </person-group>
          <article-title>The reception of environmental sounds through wearable tactual aids</article-title>
          <source>
            <italic>Ear and Hearing</italic>
          </source>
          <year>2003</year>
          <volume>24</volume>
          <issue>6</issue>
          <fpage>528</fpage>
          <lpage>538</lpage>
          <pub-id pub-id-type="other">2-s2.0-0345599130</pub-id>
          <pub-id pub-id-type="pmid">14663352</pub-id>
        </element-citation>
      </ref>
      <ref id="B16">
        <label>14</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>R&#xF6;nnberg</surname>
              <given-names>J</given-names>
            </name>
            <name>
              <surname>Samuelsson</surname>
              <given-names>E</given-names>
            </name>
            <name>
              <surname>Borg</surname>
              <given-names>E</given-names>
            </name>
          </person-group>
          <article-title>Exploring the perceived world of the deaf-blind: on the development of an instrument</article-title>
          <source>
            <italic>International Journal of Audiology</italic>
          </source>
          <year>2002</year>
          <volume>41</volume>
          <issue>2</issue>
          <fpage>136</fpage>
          <lpage>143</lpage>
          <pub-id pub-id-type="other">2-s2.0-0242636159</pub-id>
          <pub-id pub-id-type="pmid">12212859</pub-id>
        </element-citation>
      </ref>
      <ref id="B9">
        <label>15</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Otterstedde</surname>
              <given-names>CR</given-names>
            </name>
            <name>
              <surname>Spandau</surname>
              <given-names>U</given-names>
            </name>
            <name>
              <surname>Blankenagel</surname>
              <given-names>A</given-names>
            </name>
            <name>
              <surname>Kimberling</surname>
              <given-names>WJ</given-names>
            </name>
            <name>
              <surname>Reisser</surname>
              <given-names>C</given-names>
            </name>
          </person-group>
          <article-title>A new clinical classification for Usher&#x2019;s syndrome based on a new subtype of Usher&#x2019;s syndrome type I</article-title>
          <source>
            <italic>Laryngoscope</italic>
          </source>
          <year>2001</year>
          <volume>111</volume>
          <issue>1</issue>
          <fpage>84</fpage>
          <lpage>86</lpage>
          <pub-id pub-id-type="other">2-s2.0-0035166905</pub-id>
          <pub-id pub-id-type="pmid">11192904</pub-id>
        </element-citation>
      </ref>
      <ref id="B7">
        <label>16</label>
        <element-citation publication-type="confproc">
          <person-group person-group-type="author">
            <name>
              <surname>Koubek</surname>
              <given-names>RJ</given-names>
            </name>
            <name>
              <surname>Phillips</surname>
              <given-names>CA</given-names>
            </name>
            <name>
              <surname>Allread</surname>
              <given-names>WG</given-names>
            </name>
            <name>
              <surname>Barnaba</surname>
              <given-names>JE</given-names>
            </name>
            <name>
              <surname>McClain</surname>
              <given-names>JE</given-names>
            </name>
          </person-group>
          <article-title>The effects of training on a vibrotacticle cognitive feedback system</article-title>
          <conf-name>Proceedings of the Annual International Conference of the IEEE Engineering in Engineering in Medicine and Biology Society, Images of the 21st Century</conf-name>
          <conf-date>November 1989</conf-date>
          <fpage>1518</fpage>
          <lpage>1519</lpage>
          <pub-id pub-id-type="other">2-s2.0-0024765871</pub-id>
        </element-citation>
      </ref>
      <ref id="B4">
        <label>17</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Eisenwort</surname>
              <given-names>B</given-names>
            </name>
            <name>
              <surname>Benko</surname>
              <given-names>K</given-names>
            </name>
          </person-group>
          <article-title>Communicative competence in hearing-impaired. First version of a training and test program for cohclear implant patients and hearing aid patients</article-title>
          <source>
            <italic>Folia Phoniatrica</italic>
          </source>
          <year>1983</year>
          <volume>35</volume>
          <issue>6</issue>
          <fpage>273</fpage>
          <lpage>285</lpage>
          <pub-id pub-id-type="other">2-s2.0-0021050006</pub-id>
          <pub-id pub-id-type="pmid">6689161</pub-id>
        </element-citation>
      </ref>
      <ref id="B5">
        <label>18</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Gagne</surname>
              <given-names>J-P</given-names>
            </name>
            <name>
              <surname>Parnes</surname>
              <given-names>LS</given-names>
            </name>
            <name>
              <surname>LaRocque</surname>
              <given-names>M</given-names>
            </name>
            <name>
              <surname>Hassan</surname>
              <given-names>R</given-names>
            </name>
            <name>
              <surname>Vidas</surname>
              <given-names>S</given-names>
            </name>
          </person-group>
          <article-title>Effectiveness of an intensive speech perception training program for adult cochlear implant recipients</article-title>
          <source>
            <italic>Annals of Otology, Rhinology and Laryngology</italic>
          </source>
          <year>1991</year>
          <volume>100</volume>
          <issue>9, part 1</issue>
          <fpage>700</fpage>
          <lpage>707</lpage>
          <pub-id pub-id-type="other">2-s2.0-0025951785</pub-id>
        </element-citation>
      </ref>
      <ref id="B8">
        <label>19</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Looi</surname>
              <given-names>V</given-names>
            </name>
            <name>
              <surname>She</surname>
              <given-names>J</given-names>
            </name>
          </person-group>
          <article-title>Music perception of cochlear implant users: a questionnaire, and its implications for a music training program</article-title>
          <source>
            <italic>International Journal of Audiology</italic>
          </source>
          <year>2010</year>
          <volume>49</volume>
          <issue>2</issue>
          <fpage>116</fpage>
          <lpage>128</lpage>
          <pub-id pub-id-type="other">2-s2.0-77149134347</pub-id>
          <pub-id pub-id-type="pmid">20151886</pub-id>
        </element-citation>
      </ref>
      <ref id="B6">
        <label>20</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Gatehouse</surname>
              <given-names>S</given-names>
            </name>
            <name>
              <surname>Naylor</surname>
              <given-names>G</given-names>
            </name>
            <name>
              <surname>Elberling</surname>
              <given-names>C</given-names>
            </name>
          </person-group>
          <article-title>Benefits from hearing aids in relation to the interaction between the user and the environment</article-title>
          <source>
            <italic>International Journal of Audiology</italic>
          </source>
          <year>2003</year>
          <volume>42</volume>
          <issue>supplement 1</issue>
          <fpage>S77</fpage>
          <lpage>S85</lpage>
          <pub-id pub-id-type="other">2-s2.0-0642343306</pub-id>
          <pub-id pub-id-type="pmid">12918613</pub-id>
        </element-citation>
      </ref>
      <ref id="B19">
        <label>21</label>
        <element-citation publication-type="book">
          <person-group person-group-type="editor">
            <name>
              <surname>Summers</surname>
              <given-names>IR</given-names>
            </name>
          </person-group>
          <source>
            <italic>Tactile Aids for the Hearing Impaired</italic>
          </source>
          <year>1992</year>
          <publisher-loc>London</publisher-loc>
          <publisher-name>Whurr Publishers</publisher-name>
        </element-citation>
      </ref>
    </ref-list>
  </back>
  <floats-group>
    <fig id="fig1" orientation="portrait" position="float">
      <label>Figure 1</label>
      <caption>
        <p>(a) Monitor and its parts, cell phone with the application, microphone, vibrator, and amplifier. (b) The P has the cell phone and the amplifier in the armband, the microphone on the dorsal side of the hand under the sweatband, and the vibrator on the palmar side of the hand under the sweatband. The P uses her right hand to feel the vibrations better.</p>
      </caption>
      <graphic xlink:href="TSWJ2013-206734.001"/>
    </fig>
    <fig id="fig2" orientation="portrait" position="float">
      <label>Figure 2</label>
      <caption>
        <p>The sensitivity of the Phonac MM8 microphone in two different settings, omnidirectional and directional (adapted with permission from Phonac).</p>
      </caption>
      <graphic xlink:href="TSWJ2013-206734.002"/>
    </fig>
    <fig id="fig3" orientation="portrait" position="float">
      <label>Figure 3</label>
      <caption>
        <p>Frequency response of the vibrator C2-tactor. Adapted by permission from <ext-link ext-link-type="uri" xlink:href="http://www.tactors.com/">http://www.tactors.com/</ext-link>.</p>
      </caption>
      <graphic xlink:href="TSWJ2013-206734.003"/>
    </fig>
    <fig id="fig4" orientation="portrait" position="float">
      <label>Figure 4</label>
      <caption>
        <p>Detection and identification scores of events occurring in the home environment for four participants with Usher's Syndrome type I, when they were tested without Monitor (noM), with Monitor with omnidirectional microphone (MO), and with Monitor with directional microphone (MD).</p>
      </caption>
      <graphic xlink:href="TSWJ2013-206734.004"/>
    </fig>
    <fig id="fig5" orientation="portrait" position="float">
      <label>Figure 5</label>
      <caption>
        <p>Detection and identification scores of events occurring in a traffic environment for four participants with Usher's Syndrome type I, when they were tested without Monitor (noM), with Monitor with omnidirectional microphone (MO), and with Monitor with directional microphone (MD). In the noM situation the Ps only detected the events coming from behind when the object had passed them.</p>
      </caption>
      <graphic xlink:href="TSWJ2013-206734.005"/>
    </fig>
    <fig id="fig6" orientation="portrait" position="float">
      <label>Figure 6</label>
      <caption>
        <p>Illustration of a person with tunnel vision in a traffic situation. The shaded area shows the visual field of the P. Objects marked with a cross and ring are not detected while objects marked with a cross are detected. (a) Without Monitor (noM). (b) With Monitor with omnidirectional microphone (MO). (c) With Monitor with directional microphone (MD).</p>
      </caption>
      <graphic xlink:href="TSWJ2013-206734.006"/>
    </fig>
    <table-wrap id="tab1" orientation="portrait" position="float">
      <label>Table 1</label>
      <caption>
        <p>Anamnestic information of the four female participants with Usher's Syndrome I (P1, P2, P3, and P4).</p>
      </caption>
      <table frame="hsides" rules="groups">
        <thead>
          <tr>
            <th align="left" rowspan="1" colspan="1">Participant</th>
            <th align="center" rowspan="1" colspan="1">P1</th>
            <th align="center" rowspan="1" colspan="1">P2</th>
            <th align="center" rowspan="1" colspan="1">P3</th>
            <th align="center" rowspan="1" colspan="1">P4</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td align="left" rowspan="1" colspan="1">Age</td>
            <td align="center" rowspan="1" colspan="1">50</td>
            <td align="center" rowspan="1" colspan="1">44</td>
            <td align="center" rowspan="1" colspan="1">51</td>
            <td align="center" rowspan="1" colspan="1">54</td>
          </tr>
          <tr>
            <td align="left" rowspan="1" colspan="1">Visual Acuity (right eye)</td>
            <td align="center" rowspan="1" colspan="1">Light perception (yr 2009)</td>
            <td align="center" rowspan="1" colspan="1">0.1&#x2009;cc (yr 2011)</td>
            <td align="center" rowspan="1" colspan="1">0.3&#x2009;cc (yr 2013)</td>
            <td align="center" rowspan="1" colspan="1">0.16&#x2009;cc (yr 2011)</td>
          </tr>
          <tr>
            <td align="left" rowspan="1" colspan="1">Visual Acuity (left eye)</td>
            <td align="center" rowspan="1" colspan="1">Light perception (yr 2009)</td>
            <td align="center" rowspan="1" colspan="1">0.09&#x2009;cc (yr 2011)</td>
            <td align="center" rowspan="1" colspan="1">0.4&#x2009;cc (yr 2012)</td>
            <td align="center" rowspan="1" colspan="1">0.16&#x2009;cc (yr 2011)</td>
          </tr>
          <tr>
            <td align="left" rowspan="1" colspan="1">Visual field, right eye (Goldman, V/4 obj.)</td>
            <td align="center" rowspan="1" colspan="1">&lt;2&#xB0;(yr 2009)</td>
            <td align="center" rowspan="1" colspan="1">5&#xB0; (yr 2011)</td>
            <td align="center" rowspan="1" colspan="1">&lt;10&#xB0; (yr 2012)</td>
            <td align="center" rowspan="1" colspan="1">10&#xB0; (yr 2011)</td>
          </tr>
          <tr>
            <td align="left" rowspan="1" colspan="1">Visual field, left eye (Goldman, V/4 obj.)</td>
            <td align="center" rowspan="1" colspan="1">&lt;5&#xB0; (yr 2009)</td>
            <td align="center" rowspan="1" colspan="1">5&#xB0; (yr 2011)</td>
            <td align="center" rowspan="1" colspan="1">&lt;10&#xB0; (yr 2012)</td>
            <td align="center" rowspan="1" colspan="1">10&#xB0; (yr 2011)</td>
          </tr>
          <tr>
            <td align="left" rowspan="1" colspan="1">Age of subjective notified visual impairment</td>
            <td align="center" rowspan="1" colspan="1">About eight</td>
            <td align="center" rowspan="1" colspan="1">Teens</td>
            <td align="center" rowspan="1" colspan="1">Teens</td>
            <td align="center" rowspan="1" colspan="1">Teens</td>
          </tr>
          <tr>
            <td align="left" rowspan="1" colspan="1">Hearing</td>
            <td align="center" rowspan="1" colspan="1">Born deaf</td>
            <td align="center" rowspan="1" colspan="1">Born deaf</td>
            <td align="center" rowspan="1" colspan="1">Born deaf (used hearing aid)</td>
            <td align="center" rowspan="1" colspan="1">Born deaf</td>
          </tr>
          <tr>
            <td align="left" rowspan="1" colspan="1">Ways of communication</td>
            <td align="center" rowspan="1" colspan="1">Tactile sign language, E-mail, and SMS and, braille</td>
            <td align="center" rowspan="1" colspan="1">Visual and Tactile sign language, E-mail, SMS, reading, and writing</td>
            <td align="center" rowspan="1" colspan="1">Visual sign language, E-mail SMS, reading, and writing,</td>
            <td align="center" rowspan="1" colspan="1">Visual sign language, E-mail SMS, reading, and writing,</td>
          </tr>
        </tbody>
      </table>
    </table-wrap>
    <table-wrap id="tab2" orientation="portrait" position="float">
      <label>Table 2</label>
      <caption>
        <p>The four algorithms used to process the sounds in the laboratory study.</p>
      </caption>
      <table frame="hsides" rules="groups">
        <thead>
          <tr>
            <th align="left" rowspan="1" colspan="1">Abbreviation</th>
            <th align="left" rowspan="1" colspan="1">Description</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td align="left" rowspan="1" colspan="1">AM</td>
            <td align="left" rowspan="1" colspan="1">Amplitude modulation of a 250&#x2009;Hz carrier wave.</td>
          </tr>
          <tr>
            <td align="left" rowspan="1" colspan="1">AMMC</td>
            <td align="left" rowspan="1" colspan="1">Amplitude modulation with multiple channels.</td>
          </tr>
          <tr>
            <td align="left" rowspan="1" colspan="1">TR</td>
            <td align="left" rowspan="1" colspan="1">Transferring data in the range of 0&#x2013;5500&#x2009;Hz to the range of 0&#x2013;290&#x2009;Hz.</td>
          </tr>
          <tr>
            <td align="left" rowspan="1" colspan="1">TRHA</td>
            <td align="left" rowspan="1" colspan="1">Transposing the 10 frequency components with highest amplitude in the range of 0&#x2013;5500&#x2009;Hz to the range of 52&#x2013;470&#x2009;Hz.</td>
          </tr>
        </tbody>
      </table>
    </table-wrap>
    <table-wrap id="tab3" orientation="portrait" position="float">
      <label>Table 3</label>
      <caption>
        <p>Sounds from events used in the tests in home and traffic environments.</p>
      </caption>
      <table frame="hsides" rules="groups">
        <thead>
          <tr>
            <th align="left" rowspan="1" colspan="1">Number</th>
            <th align="left" rowspan="1" colspan="1">Sounds from events in home environment</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td align="left" rowspan="1" colspan="1">(1)</td>
            <td align="left" rowspan="1" colspan="1">Doorbell</td>
          </tr>
          <tr>
            <td align="left" rowspan="1" colspan="1">(2)</td>
            <td align="left" rowspan="1" colspan="1">Water flushing</td>
          </tr>
          <tr>
            <td align="left" rowspan="1" colspan="1">(3)</td>
            <td align="left" rowspan="1" colspan="1">Telephone signaling</td>
          </tr>
          <tr>
            <td align="left" rowspan="1" colspan="1">(4)</td>
            <td align="left" rowspan="1" colspan="1">Toilet flushing</td>
          </tr>
          <tr>
            <td align="left" rowspan="1" colspan="1">(5)</td>
            <td align="left" rowspan="1" colspan="1">Door opening and closing</td>
          </tr>
          <tr>
            <td align="left" rowspan="1" colspan="1">(6)</td>
            <td align="left" rowspan="1" colspan="1">Popping popcorn in a microwave oven</td>
          </tr>
          <tr>
            <td align="left" rowspan="1" colspan="1">(7)</td>
            <td align="left" rowspan="1" colspan="1">Vacuum cleaner</td>
          </tr>
          <tr>
            <td align="left" rowspan="1" colspan="1">(8)</td>
            <td align="left" rowspan="1" colspan="1">A person talking</td>
          </tr>
          <tr>
            <td align="left" rowspan="1" colspan="1">(9)</td>
            <td align="left" rowspan="1" colspan="1">Coffee maker</td>
          </tr>
          <tr>
            <td align="left" rowspan="1" colspan="1">(10)</td>
            <td align="left" rowspan="1" colspan="1">Talk and music from TV</td>
          </tr>
          <tr>
            <td align="left" rowspan="1" colspan="1">(11)</td>
            <td align="left" rowspan="1" colspan="1">Dropping keys</td>
          </tr>
          <tr>
            <td align="left" rowspan="1" colspan="1">(12)</td>
            <td align="left" rowspan="1" colspan="1">Footsteps</td>
          </tr>
          <tr>
            <td align="left" rowspan="1" colspan="1">(13)</td>
            <td align="left" rowspan="1" colspan="1">Heavy traffic from window</td>
          </tr>
          <tr>
            <td align="left" rowspan="1" colspan="1">(14)</td>
            <td align="left" rowspan="1" colspan="1">Water boiling</td>
          </tr>
          <tr>
            <td align="left" rowspan="1" colspan="1">(15)</td>
            <td align="left" rowspan="1" colspan="1">A person coughing</td>
          </tr>
          <tr>
            <td align="left" colspan="2" rowspan="1">
<hr/>
</td>
          </tr>
          <tr>
            <td align="left" rowspan="1" colspan="1">Number</td>
            <td align="left" rowspan="1" colspan="1">Sounds from events in traffic environment</td>
          </tr>
          <tr>
            <td align="left" colspan="2" rowspan="1">
<hr/>
</td>
          </tr>
          <tr>
            <td align="left" rowspan="1" colspan="1">(1)</td>
            <td align="left" rowspan="1" colspan="1">Bicycle passing from behind with/without signaling </td>
          </tr>
          <tr>
            <td align="left" rowspan="1" colspan="1">(2)</td>
            <td align="left" rowspan="1" colspan="1">Bicycle coming towards P with/without signaling</td>
          </tr>
          <tr>
            <td align="left" rowspan="1" colspan="1">(3)</td>
            <td align="left" rowspan="1" colspan="1">Car passing from behind with/without signaling</td>
          </tr>
          <tr>
            <td align="left" rowspan="1" colspan="1">(4)</td>
            <td align="left" rowspan="1" colspan="1">Car coming towards P with/without signaling</td>
          </tr>
          <tr>
            <td align="left" rowspan="1" colspan="1">(5)</td>
            <td align="left" rowspan="1" colspan="1">A person running from behind to front</td>
          </tr>
          <tr>
            <td align="left" rowspan="1" colspan="1">(6)</td>
            <td align="left" rowspan="1" colspan="1">A talking person walking from behind to front</td>
          </tr>
        </tbody>
      </table>
    </table-wrap>
    <table-wrap id="tab4" orientation="portrait" position="float">
      <label>Table 4</label>
      <caption>
        <p>The number of training days and average training hours/day (digits in the parenthesis) for each P at two-training period.</p>
      </caption>
      <table frame="hsides" rules="groups">
        <thead>
          <tr>
            <th align="left" rowspan="1" colspan="1">&#x2009;</th>
            <th align="center" rowspan="1" colspan="1">P1</th>
            <th align="center" rowspan="1" colspan="1">P2</th>
            <th align="center" rowspan="1" colspan="1">P3</th>
            <th align="center" rowspan="1" colspan="1">P4</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td align="left" rowspan="1" colspan="1">Five weeks training with MO</td>
            <td align="center" rowspan="1" colspan="1">109 (9.6)</td>
            <td align="center" rowspan="1" colspan="1">33 (4.0)</td>
            <td align="center" rowspan="1" colspan="1">25 (5.6)</td>
            <td align="center" rowspan="1" colspan="1">57 (8.9)</td>
          </tr>
          <tr>
            <td align="left" rowspan="1" colspan="1">Five weeks training with MD</td>
            <td align="center" rowspan="1" colspan="1">33 (11.1)</td>
            <td align="center" rowspan="1" colspan="1">21 (7.0)</td>
            <td align="center" rowspan="1" colspan="1">32 (2.7)</td>
            <td align="center" rowspan="1" colspan="1">35 (10.7)</td>
          </tr>
        </tbody>
      </table>
    </table-wrap>
  </floats-group>
</article>
</pmc-articleset>
