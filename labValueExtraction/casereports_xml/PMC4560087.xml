<?xml version="1.0" ?>
<!DOCTYPE pmc-articleset PUBLIC "-//NLM//DTD ARTICLE SET 2.0//EN" "https://dtd.nlm.nih.gov/ncbi/pmc/articleset/nlm-articleset-2.0.dtd">
<pmc-articleset><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article">
  <?properties open_access?>
  <front>
    <journal-meta>
      <journal-id journal-id-type="nlm-ta">J Neuroeng Rehabil</journal-id>
      <journal-id journal-id-type="iso-abbrev">J Neuroeng Rehabil</journal-id>
      <journal-title-group>
        <journal-title>Journal of NeuroEngineering and Rehabilitation</journal-title>
      </journal-title-group>
      <issn pub-type="epub">1743-0003</issn>
      <publisher>
        <publisher-name>BioMed Central</publisher-name>
        <publisher-loc>London</publisher-loc>
      </publisher>
    </journal-meta>
    <article-meta>
      <article-id pub-id-type="pmid">26338101</article-id>
      <article-id pub-id-type="pmc">4560087</article-id>
      <article-id pub-id-type="publisher-id">71</article-id>
      <article-id pub-id-type="doi">10.1186/s12984-015-0071-z</article-id>
      <article-categories>
        <subj-group subj-group-type="heading">
          <subject>Research</subject>
        </subj-group>
      </article-categories>
      <title-group>
        <article-title>Comparison of eye tracking, electrooculography and an auditory brain-computer interface for binary communication: a case study with a participant in the locked-in state</article-title>
      </title-group>
      <contrib-group>
        <contrib contrib-type="author" corresp="yes">
          <name>
            <surname>K&#xE4;thner</surname>
            <given-names>Ivo</given-names>
          </name>
          <address>
            <email>ivo.kaethner@uni-wuerzburg.de</email>
          </address>
          <xref ref-type="aff" rid="Aff1"/>
        </contrib>
        <contrib contrib-type="author">
          <name>
            <surname>K&#xFC;bler</surname>
            <given-names>Andrea</given-names>
          </name>
          <address>
            <email>andrea.kuebler@uni-wuerzburg.de</email>
          </address>
          <xref ref-type="aff" rid="Aff1"/>
        </contrib>
        <contrib contrib-type="author">
          <name>
            <surname>Halder</surname>
            <given-names>Sebastian</given-names>
          </name>
          <address>
            <email>sebastian.halder@uni-wuerzburg.de</email>
          </address>
          <xref ref-type="aff" rid="Aff1"/>
          <xref ref-type="aff" rid="Aff2"/>
        </contrib>
        <aff id="Aff1"><label/>Institute of Psychology, University of W&#xFC;rzburg, Marcusstr. 9-11, 97070 W&#xFC;rzburg, Germany </aff>
        <aff id="Aff2"><label/>Department of Rehabilitation for Brain Functions, Research Institute of National Rehabilitation Center for Persons with Disabilities, 4-1 Namiki, Tokorozawa, Saitama 359-8555 Japan </aff>
      </contrib-group>
      <pub-date pub-type="epub">
        <day>4</day>
        <month>9</month>
        <year>2015</year>
      </pub-date>
      <pub-date pub-type="pmc-release">
        <day>4</day>
        <month>9</month>
        <year>2015</year>
      </pub-date>
      <pub-date pub-type="collection">
        <year>2015</year>
      </pub-date>
      <volume>12</volume>
      <elocation-id>76</elocation-id>
      <history>
        <date date-type="received">
          <day>20</day>
          <month>5</month>
          <year>2015</year>
        </date>
        <date date-type="accepted">
          <day>27</day>
          <month>8</month>
          <year>2015</year>
        </date>
      </history>
      <permissions>
        <copyright-statement>&#xA9; K&#xE4;thner et al. 2015</copyright-statement>
        <license license-type="OpenAccess">
          <license-p>
<bold>Open Access</bold>This article is distributed under the terms of the Creative Commons Attribution 4.0 International License (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted use, distribution, and reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/publicdomain/zero/1.0/">http://creativecommons.org/publicdomain/zero/1.0/</ext-link>) applies to the data made available in this article, unless otherwise stated.</license-p>
        </license>
      </permissions>
      <abstract id="Abs1">
        <sec>
          <title>Background</title>
          <p>In this study, we evaluated electrooculography (EOG), an eye tracker and an auditory brain-computer interface (BCI) as access methods to augmentative and alternative communication (AAC). The participant of the study has been in the locked-in state (LIS) for 6&#xA0;years due to amyotrophic lateral sclerosis. He was able to communicate with slow residual eye movements, but had no means of partner independent communication. We discuss the usability of all tested access methods and the prospects of using BCIs as an assistive technology.</p>
        </sec>
        <sec>
          <title>Methods</title>
          <p>Within four days, we tested whether EOG, eye tracking and a BCI would allow the participant in LIS to make simple selections. We optimized the parameters in an iterative procedure for all systems.</p>
        </sec>
        <sec>
          <title>Results</title>
          <p>The participant was able to gain control over all three systems. Nonetheless, due to the level of proficiency previously achieved with his low-tech AAC method, he did not consider using any of the tested systems as an additional communication channel. However, he would consider using the BCI once control over his eye muscles would no longer be possible. He rated the ease of use of the BCI as the highest among the tested systems, because no precise eye movements were required; but also as the most tiring, due to the high level of attention needed to operate the BCI.</p>
        </sec>
        <sec>
          <title>Conclusions</title>
          <p>In this case study, the partner based communication was possible due to the good care provided and the proficiency achieved by the interlocutors. To ease the transition from a low-tech AAC method to a BCI once control over all muscles is lost, it must be simple to operate. For persons, who rely on AAC and are affected by a progressive neuromuscular disease, we argue that a complementary approach, combining BCIs and standard assistive technology, can prove valuable to achieve partner independent communication and ease the transition to a purely BCI based approach. Finally, we provide further evidence for the importance of a user-centered approach in the design of new assistive devices.</p>
        </sec>
        <sec>
          <title>Electronic supplementary material</title>
          <p>The online version of this article (doi:10.1186/s12984-015-0071-z) contains supplementary material, which is available to authorized users.</p>
        </sec>
      </abstract>
      <custom-meta-group>
        <custom-meta>
          <meta-name>issue-copyright-statement</meta-name>
          <meta-value>&#xA9; The Author(s) 2015</meta-value>
        </custom-meta>
      </custom-meta-group>
    </article-meta>
  </front>
  <body>
    <sec id="Sec1" sec-type="introduction">
      <title>Background</title>
      <p>Neurodegenerative diseases such as amyotrophic lateral sclerosis (ALS) or lesions in the brainstem caused by stroke, traumatic or anoxic brain injury can lead to a locked-in syndrome (LIS). First coined by Plum and Posner in 1966 [<xref ref-type="bibr" rid="CR1">1</xref>], the term describes a state in which persons are severely paralyzed (quadriparesis or quadriplegia), unable to speak (aphonia or severe dysphonia), but aware of their environment and show cognitive abilities on examination. Eye opening is sustained and the principle methods of communication are vertical or horizontal eye-movements or blinking (according to the criteria suggested by the American Congress of Rehabilitation Medicine [<xref ref-type="bibr" rid="CR2">2</xref>]). The term total (or complete) LIS refers to a state of complete motor paralysis with no control over eye movements [<xref ref-type="bibr" rid="CR3">3</xref>, <xref ref-type="bibr" rid="CR4">4</xref>].</p>
      <p>Persons in the locked-in state require augmentative and alternative communication (AAC) to replace speech. These AAC systems can range from communication based on eye movements during face-to-face communication with caregivers to technical aids that allow for communication independent of the caregiver. While there are several high-tech communication aids available such as dynamic touch screens for persons who have residual muscular control [<xref ref-type="bibr" rid="CR5">5</xref>], access options to AAC for persons in LIS are sparse. One of the most promising options is an eye-tracking based approach. A recent survey among 30 persons with advanced ALS using an eye-tracking computer system showed a high acceptance and average daily usage of 300&#xA0;min of the device [<xref ref-type="bibr" rid="CR6">6</xref>]. However, almost every fourth participant of the study (<italic>n</italic>&#x2009;=&#x2009;7) reported a low daily utilization. Eyestrain and the inability to move the eyes sufficiently precise were the most frequent reasons reported for non-use. Another access option to AAC repeatedly proposed uses electrodes placed around the eyes of the user to record the electrooculogram (EOG) and thereby identify eye movements and/or blinks [<xref ref-type="bibr" rid="CR7">7</xref>&#x2013;<xref ref-type="bibr" rid="CR9">9</xref>]. Similar to the eye tracker, this method relies on the users&#x2019; abilities to control their eye-muscles.</p>
      <p>Brain-computer interfaces (BCIs) can provide a muscle-independent communication channel (for reviews, [<xref ref-type="bibr" rid="CR10">10</xref>, <xref ref-type="bibr" rid="CR11">11</xref>]). A BCI based on event-related potentials (ERPs) in the electroencephalogram (EEG) was first proposed by Farwell and Donchin (1988 [<xref ref-type="bibr" rid="CR12">12</xref>]) and ERPs are now the most widely applied control signals to enable communication with a BCI (for reviews see [<xref ref-type="bibr" rid="CR13">13</xref>, <xref ref-type="bibr" rid="CR14">14</xref>]). Usually ERPs are elicited in so-called oddball paradigms. The users have the task of attending rare (odd) target stimuli in a series of frequent stimuli. These rare target stimuli elicit specific ERPs that can be classified and translated into computer commands. The most prominent among the elicited ERPs is the P300. It is a positive deflection in the EEG that occurs approximately 200&#x2013;500&#xA0;ms after the onset of a rare attended target stimulus with maximum amplitudes over central and parietal areas of the scalp [<xref ref-type="bibr" rid="CR15">15</xref>]. For many years research focused on BCIs that apply visual stimulation to elicit ERPs. Most healthy users are able to control visual ERP-BCIs with high accuracies [<xref ref-type="bibr" rid="CR16">16</xref>] and persons with severe paralysis were able to gain control over it [<xref ref-type="bibr" rid="CR17">17</xref>&#x2013;<xref ref-type="bibr" rid="CR19">19</xref>]. Even control over complex applications such as a web browser, multimedia player and a painting application and long-term independent home use by ALS patients have been demonstrated [<xref ref-type="bibr" rid="CR20">20</xref>&#x2013;<xref ref-type="bibr" rid="CR22">22</xref>]. Pasqualotto et al. [<xref ref-type="bibr" rid="CR23">23</xref>] revealed a higher performance and usability for an eye-tracking system compared to a visual BCI with a group of persons with severe motor impairment. Nevertheless, there are situations in which a BCI might be advantageous. For instance, the participant with ALS of the long-term case study by Holz et al. [<xref ref-type="bibr" rid="CR21">21</xref>] reported that it was less straining for her to make selection with the BCI, because unlike with her eye tracker, no eye blinks were required to make a selection. Because neither visual ERP-BCIs nor eye trackers can be controlled by persons with severe visual impairments and/or disability to control eye movements, e.g., persons in LIS, BCIs based on auditory and tactile stimulation were proposed in recent years (for a review see [<xref ref-type="bibr" rid="CR24">24</xref>]).</p>
      <p>Two types of auditory ERP-BCIs emerged. The first allows simple (binary) communication and is either based on attending target tones in a sequence of tones (sequential approach: e.g., [<xref ref-type="bibr" rid="CR25">25</xref>&#x2013;<xref ref-type="bibr" rid="CR28">28</xref>]) or shifting attention to one of two auditory streams (streaming approach: [<xref ref-type="bibr" rid="CR29">29</xref>&#x2013;<xref ref-type="bibr" rid="CR31">31</xref>]). Binary auditory BCIs are particularly suited for re-establishing simple communication with severely paralyzed persons since attentional and working-memory demands are low. The second type, multi-class BCIs, enable persons with long attention spans and good cognitive abilities to control spelling applications. For this, the number of tones to be differentiated in a sequential approach are increased ([<xref ref-type="bibr" rid="CR32">32</xref>&#x2013;<xref ref-type="bibr" rid="CR38">38</xref>]).</p>
      <p>The participant of the current study was in the locked-in state due to amyotrophic lateral sclerosis. At the time of the study, he did not use any AAC that would allow him to communicate independent of his caregivers and has not been using such technology previously. His family contacted us, because he had read about the possibility of using EOG and brain-computer interfaces as a method of communication in the study by Kaufmann et al. [<xref ref-type="bibr" rid="CR8">8</xref>]. Although he could still communicate via eye movements, he wanted to test these methods as an alternative, since he had noticed a decline in his ability to control his eye movements. Hence, the aim of the study was to test a gaze independent BCI system that he could control without muscle activity and an EOG system as an alternative to his current method. We compared these systems to an eye tracking system. The advantages and disadvantages of each system are discussed, the prospects of using BCIs as assistive technology are reviewed and the need is emphasized for user-centered design in AAC in general and BCI development in particular.</p>
    </sec>
    <sec id="Sec2" sec-type="materials|methods">
      <title>Methods</title>
      <sec id="Sec3">
        <title>Participant</title>
        <p>At the time of the study, the Norwegian participant was 55&#xA0;years old and has been in the locked-in state for 6&#xA0;years. He was diagnosed with amyotrophic lateral sclerosis 9&#xA0;years and 2&#xA0;months prior to the study with first symptoms occurring 5&#xA0;months prior to the diagnosis (muscle weakness in his legs). He was able to move slowly his eyes vertically and horizontally. Voluntary blinking was not possible. As the cause of an accident, he had lost hearing of his right ear. He was artificially ventilated (tracheostomy mechanical ventilation) and fed (percutaneous endoscopic gastrostomy). Full-time care was provided in his home, where the study was conducted.</p>
      </sec>
      <sec id="Sec4">
        <title>Conventional communication</title>
        <p>To communicate with caregivers and family members the study participant relied on a letter board. The same groups of letters were printed on both sides of a cardboard frame. The caregiver or family member held the frame, facing the user. The user could then select letters with a two-step procedure. Via eye gaze he first selected a group of letters. The caregiver read out the letters of the selected group one at a time and the user indicated the letter he wanted to spell by slightly twitching his left eyebrow, when the desired letter was read. If control of eye movements was not possible due to fatigue, the first step was also done with partner assisted scanning (i.e., the caregiver pointed to the groups of letters one after the other and the user selected a group with a short twitch of his eye).</p>
        <p>About 7&#xA0;years prior to the study, the user had tried an eye tracking based system with Rolltalk communication software (Abilia AB, Sweden), but communication had worked better with the letter board described above.</p>
      </sec>
      <sec id="Sec5">
        <title>Procedure</title>
        <p>On four consecutive days, the participant tested an EOG based system (3 sessions), an eye tracking based approach (1 session) and an auditory BCI (3 sessions) for communication. Since the participant was particularly interested in the EOG as an alternative communication channel, we started testing this system followed by the eye tracking and tested the BCI last. For all systems we optimized the parameters in a stepwise procedure to allow the participant to gain control over them. The procedure for each system is described below. Main parameters for each session are listed in Table&#xA0;<xref rid="Tab1" ref-type="table">1</xref>. During the measurements, the participant sat in his wheelchair in a reclining position. He gave informed consent prior to participation (a signature stamp was used by his caregivers). The study was carried out in accordance with the guidelines of the Declaration of Helsinki.<table-wrap id="Tab1"><label>Table 1</label><caption><p>Parameters used and selections made during the measurements</p></caption><table frame="hsides" rules="groups"><thead><tr><th/><th>Session</th><th>Sequences</th><th>Possible choices</th><th>Selections</th></tr></thead><tbody><tr><td>EOG</td><td/><td/><td/><td/></tr><tr><td>&#x2009;&#x2009;&#x2009;&#x2009;Day 1</td><td/><td/><td/><td/></tr><tr><td colspan="5">&#x2009;&#x2009;&#x2009;&#x2009;&#x2009;&#x2009;
<italic>- control signal: looking to the left and back to the center</italic>
</td></tr><tr><td colspan="5">&#x2009;&#x2009;&#x2009;&#x2009;&#x2009;&#x2009;
<italic>- one electrode placed next to the outer canthus of the left eye</italic>
</td></tr><tr><td colspan="4">&#x2009;&#x2009;&#x2009;&#x2009;&#x2009;&#x2009;
<italic>- 1000&#xA0;ms classification window</italic>
</td><td/></tr><tr><td/><td>
<italic>Training</italic>
</td><td>
<italic>10</italic>
</td><td>
<italic>2</italic>
</td><td>
<italic>2 runs</italic>
</td></tr><tr><td/><td>1.1</td><td>1</td><td>2</td><td>6</td></tr><tr><td colspan="4">&#x2009;&#x2009;&#x2009;&#x2009;&#x2009;&#x2009;
<italic>- classification window changed to 2000&#xA0;ms</italic>
</td><td/></tr><tr><td/><td>1.2</td><td>1</td><td>2</td><td>14</td></tr><tr><td/><td>
<italic>Training</italic>
</td><td colspan="3">
<italic>- data from 1.2 used to train new classifer</italic>
</td></tr><tr><td/><td>1.3</td><td>1</td><td>2</td><td>5</td></tr><tr><td/><td>1.4</td><td>1</td><td>5 (A, B, C, D, E)</td><td>5</td></tr><tr><td colspan="4">&#x2009;&#x2009;&#x2009;&#x2009;&#x2009;&#x2009;
<italic>-voice recording by daughter</italic>
</td><td/></tr><tr><td/><td>1.5</td><td>1</td><td>5 (F, G, H, I, J)</td><td>15</td></tr><tr><td colspan="4">&#x2009;&#x2009;&#x2009;&#x2009;&#x2009;&#x2009;
<italic>- added second electrode (next to canthus of right eye)</italic>
</td><td/></tr><tr><td colspan="5">&#x2009;&#x2009;&#x2009;&#x2009;&#x2009;&#x2009;
<italic>- new control signal: looking to the right and back to the center</italic>
</td></tr><tr><td/><td>1.6</td><td>1</td><td>5 (F, G, H, I, J)</td><td>10</td></tr><tr><td>&#x2009;&#x2009;&#x2009;&#x2009;Day 2</td><td/><td/><td/><td/></tr><tr><td colspan="4">&#x2009;&#x2009;&#x2009;&#x2009;&#x2009;&#x2009;
<italic>- stimulus presentation in alphabetical order</italic>
</td><td/></tr><tr><td colspan="5">&#x2009;&#x2009;&#x2009;&#x2009;&#x2009;&#x2009;
<italic>- 1500&#xA0;ms classification window</italic>
</td></tr><tr><td/><td>
<italic>Training</italic>
</td><td>
<italic>3</italic>
</td><td>5 (F, G, H, I, J)</td><td>
<italic>1 run</italic>
</td></tr><tr><td/><td>2</td><td>1</td><td>5 (F, G, H, I, J)</td><td>19</td></tr><tr><td>&#x2009;&#x2009;&#x2009;&#x2009;Day 3</td><td/><td/><td/><td/></tr><tr><td/><td>
<italic>Training</italic>
</td><td>
<italic>3</italic>
</td><td>5 (F, G, H, I, J)</td><td>
<italic>15</italic>
</td></tr><tr><td colspan="4">&#x2009;&#x2009;&#x2009;&#x2009;&#x2009;&#x2009;
<italic>- two step procedure to select letter group and target letter</italic>
</td><td/></tr><tr><td/><td>3.1</td><td>1</td><td>5(25)letters A to Y</td><td>12</td></tr><tr><td/><td>3.2</td><td>2</td><td>5(25)letters A to Y</td><td>12</td></tr><tr><td/><td>3.3</td><td>2</td><td>5(25)letters A to Y</td><td>4</td></tr><tr><td>Eye Tracker</td><td/><td/><td/><td/></tr><tr><td>&#x2009;&#x2009;&#x2009;&#x2009;Day 2</td><td/><td/><td/><td/></tr><tr><td/><td>2</td><td>1</td><td>2</td><td>38</td></tr><tr><td>BCI</td><td/><td/><td/><td/></tr><tr><td>&#x2009;&#x2009;&#x2009;&#x2009;Day 2</td><td/><td/><td/><td/></tr><tr><td/><td>
<italic>Training</italic>
</td><td>
<italic>20</italic>
</td><td>
<italic>2</italic>
</td><td>
<italic>8 runs</italic>
</td></tr><tr><td/><td>2.1</td><td>10</td><td>2</td><td>3</td></tr><tr><td/><td>2.2</td><td>7</td><td>2</td><td>3</td></tr><tr><td>&#x2009;&#x2009;&#x2009;&#x2009;Day 3</td><td/><td/><td/><td/></tr><tr><td/><td>
<italic>Training</italic>
</td><td>
<italic>20</italic>
</td><td>
<italic>2</italic>
</td><td>
<italic>6 runs</italic>
</td></tr><tr><td/><td>3.1</td><td>20</td><td>2</td><td>2</td></tr><tr><td/><td>3.2</td><td>7</td><td>2</td><td>3</td></tr><tr><td/><td>3.3</td><td>10</td><td>2</td><td>6</td></tr><tr><td>&#x2009;&#x2009;&#x2009;&#x2009;Day 4</td><td/><td/><td/><td/></tr><tr><td colspan="3">&#x2009;&#x2009;&#x2009;&#x2009;&#x2009;&#x2009;
<italic>- classifier from day 3 applied</italic>
</td><td/><td/></tr><tr><td/><td>4.1</td><td>20</td><td>2</td><td>4</td></tr><tr><td colspan="5">&#x2009;&#x2009;&#x2009;&#x2009;&#x2009;&#x2009;
<italic>- data from 4.1 added to data from day 3 to train new classifier</italic>
</td></tr><tr><td/><td>4.2</td><td>10</td><td>2</td><td>4</td></tr></tbody></table></table-wrap></p>
      </sec>
      <sec id="Sec6">
        <title>Data acquisition</title>
        <p>Stimulus presentation, data processing and storage were controlled by the BCI2000 software framework [<xref ref-type="bibr" rid="CR39">39</xref>]. EEG data during BCI and EOG use was amplified with a g.USBamp (g.tec, Austria) with a sampling rate of 256&#xA0;Hz, a bandpass filter from 1 to 30&#xA0;Hz and a notch filter around 50&#xA0;Hz. The EyeX eyetracker was connected to BCI2000 using the software development kit provided with the hardware by Tobii Technology. Recordings were made with a Hewlett-Packard ProBook 6460b with a dual-core CPU, 4GB of RAM and a 64-bit Windows 7.</p>
      </sec>
      <sec id="Sec7">
        <title>EOG</title>
        <p>The eye movement that the participant used to communicate during the partner scanning approach (twitching his left eyebrow) was not strong enough to be registered with an electrode placed above his left eyebrow. Thus, we asked the participant to perform another eye movement that he could control reliably and that was not too strenuous. The participant chose looking to the left and back to the center as control signal for the EOG. To record this movement we placed one electrode next to the outer canthus of his left eye. Stepwise linear discriminant analysis (SWLDA, [<xref ref-type="bibr" rid="CR40">40</xref>]) was applied to determine features and feature weights of the EOG data acquired during the training runs that were subsequently used for online classification during the feedback runs. During both the training and the feedback runs we asked the participant to respond with an eye movement to a predefined target letter. Voice recordings of the letters &#x201C;A&#x201D; and &#x201C;B&#x201D; were played in random order via the built in speakers of the notebook. Presentation of these two letters constituted one sequence. Stimulus duration was set to 1&#xA0;s and the interstimulus interval (ISI) to 2.5&#xA0;s, hence stimulus onset asynchrony (SOA) was 3.5&#xA0;s. During the feedback runs, the selected letter, &#x201C;A&#x201D; or &#x201C;B&#x201D;, was presented after a short signal tone. The pause between sequences was set to 5&#xA0;s, thus 12&#xA0;s were needed for one selection with this two choice paradigm. Later during that session the participant decided to switch to &#x201C;looking to the right and back to the center&#x201D; as control signal for the EOG. Therefore, we placed an additional electrode next to the outer canthus of his right eye and used the differential activity of both electrodes to classify his eye movements. Because eye movements of the participant were slow, we changed the time window for classification from originally 1000 to 2000&#xA0;ms during the process of testing (see Table&#xA0;<xref rid="Tab1" ref-type="table">1</xref> for an overview of the applied parameters).</p>
        <p>To increase the number of possible selections to 5 we presented the letters A, B, C, D and E. To increase the discriminability of the letter recordings (the letters D and E sounded similar), we later asked the daughter of the participant to record the letters F, G, H, I and J in Norwegian. These letters were played in random order and the participant was instructed to respond with eye movements to the target letter. On day 2, we facilitated the task by playing the letters in alphabetical order. On both days, the feedback consisted of the chosen letter that was played after a short signal tone. Selection of one letter took 22.5&#xA0;s with one sequence. On day three we asked the participant to spell few words with a two-step procedure. First, we asked the participant to select a group of letters (A-E, F-J, K-O, P-T or U-Y) and in the second step the target letter within the chosen group. The groups of letters and letters within the chosen group were each coded by the spoken numbers 1 to 5. Therefore, he had to respond to the number 1 and after a short break of 7.5&#xA0;s to the number 3 to select the letter &#x201C;C&#x201D;. To facilitate the task for the participant, the assignment of groups of letters and numbers (e.g., during the first step A, B, C, D, E&#x2009;=&#x2009;1; during the second step A&#x2009;=&#x2009;1) were presented to the participant on a sheet of paper. Feedback about the chosen group and letter was provided acoustically after every step. Selection for one step took 25&#xA0;s, hence the time needed to spell one letter was 50&#xA0;s with this paradigm.</p>
      </sec>
      <sec id="Sec8">
        <title>Eye tracking</title>
        <p>Eye movements were recorded with a Tobii EyeX Dev Kit (Tobii Technology, Sweden) that is based on the principle of corneal reflection tracking. The eye tracker was attached to a metal post using a flexible holder and positioned in front of the user such that the eyes could be recognized by the system (see Fig.&#xA0;<xref rid="Fig1" ref-type="fig">1</xref>). The area above the eye tracker (corresponding to a screen size of 1680&#x2009;&#xD7;&#x2009;1050&#xA0;pixels) constituted the area in which the gaze point could be determined. We ensured that this area was within the participant&#x2019;s field of view. To calibrate the system, we asked the participant to follow the predefined movements of a pen that we held in his field of view. To allow for a comparison with the EOG, we chose looking to the left or right as control signal. Similar to the procedure for the EOG, the letters &#x201C;A&#x201D; and &#x201C;B&#x201D; were played over speakers (one sequence). Before each run, the user was asked to respond to one of the two. If &#x201C;A&#x201D; was the designated target, the user was asked to look to the left and for &#x201C;B&#x201D; to the right. The gaze point was determined at the end of each sequence. During free runs we asked the user to respond to either of the two stimuli and asked him afterwards which selections he had aimed for. Stimulus duration was set to 1&#xA0;s and the inter-stimulus interval to 1.5&#xA0;s. Between sequences of letters there was a pause of 3&#xA0;s in which the user was instructed to look straight ahead and during which he received acoustic feedback. A voice recording saying &#x201C;left&#x201D; or &#x201C;right&#x201D; was played depending on the determined position of the gaze point (in the left or right half of the field of view). Including the time for feedback, 8&#xA0;s were needed for one selection.<fig id="Fig1"><label>Fig. 1</label><caption><p>Schematic figure depicting the position of the eye tracker in front of the user. Lateral view (<bold>a</bold>) and rear view (<bold>b</bold>). The user&#x2019;s fixation point could be determined in the area above the eye tracker. The fixation point was determined as being either in the left or right half of this area and thus, the user could make a binary choice</p></caption><graphic xlink:href="12984_2015_71_Fig1_HTML" id="MO1"/></fig></p>
      </sec>
      <sec id="Sec9">
        <title>Auditory BCI</title>
        <p>During BCI use, the EEG was recorded with 16 active Ag/AgCl electrodes mounted in an elastic fabric cap and positioned at FC3, FCz, FC4, C3, Cz, C4, CP5, CPz, CP6, P3, Pz, P4, PO5, Poz, PO6 and Oz according to the modified 10&#x2013;20 system of the American Electroencephalographic Society [<xref ref-type="bibr" rid="CR41">41</xref>]. A ground electrode was positioned at AFz and the data was referenced to an electrode clipped to the right earlobe. The EEG was amplified with a g.USBamp (g.tec, Austria) and sampled at 256&#xA0;Hz. A notch filter around 50&#xA0;Hz and a bandpass filter between 0.1 and 30&#xA0;Hz were applied. Auditory stimuli were presented over circumaural headphones (Sennheiser HD 280 pro, Germany).</p>
        <p>The task consisted of a three stimulus oddball paradigm as suggested for binary BCI communication by Halder et al. [<xref ref-type="bibr" rid="CR26">26</xref>]. Three different tones were presented in random order: A high pitched target tone with a frequency of 1000&#xA0;Hz, a low pitched target tone of 100&#xA0;Hz and a standard tone which consisted of pink noise. One sequence consisted of three standard stimuli and the two target tones. All stimuli had a duration of 80&#xA0;ms and the stimulus onset asynchrony was 1000&#xA0;ms. The participant was instructed which target tone to attend to before each run. He was asked to focus on the appearance of that tone and silently count whenever it sounded and ignore all other tones. To acquire data to train the classifier, 20 sequences were played per training run. Stepwise linear discriminant analysis [<xref ref-type="bibr" rid="CR40">40</xref>] was applied to determine features and feature weights for online selections with the BCI. During these online runs, the number of sequences was reduced and the user received feedback according to the classifier results. Hence, the time needed for one selection depended on the number of sequences (e.g., with 10 sequences: 50&#xA0;s plus 2&#xA0;s for feedback).</p>
      </sec>
      <sec id="Sec10">
        <title>Questionnaire</title>
        <p>At the end of testing, after the fourth session, we presented a summary of the achieved performance and the general advantages and disadvantages for each of the tested systems to the participant (similar to the ones summarized in Table&#xA0;<xref rid="Tab2" ref-type="table">2</xref>). To gather his feedback, we then asked him the same set of questions for all three systems. The questions are listed below. We started with the questions about the EOG and ended with the set of questions for the BCI. The participant answered all questions with his conventional communication method (partner scanning approach with eye movements).<table-wrap id="Tab2"><label>Table 2</label><caption><p>Characteristics of the tested systems</p></caption><table frame="hsides" rules="groups"><thead><tr><th/><th>EOG</th><th>Eye Tracker</th><th>BCI</th></tr></thead><tbody><tr><td>Allows communication independent of the caregiver</td><td>yes</td><td>yes</td><td>yes</td></tr><tr><td>Enables muscle-independent communication</td><td>no</td><td>no</td><td>yes</td></tr><tr><td>Speed of communication</td><td>medium</td><td>fastest</td><td>slowest</td></tr><tr><td>Commercially available</td><td/><td/><td/></tr><tr><td>&#x2022; Hardware</td><td>yes</td><td>yes</td><td>yes</td></tr><tr><td>&#x2022; AT software and support</td><td>no</td><td>yes</td><td>no</td></tr><tr><td>Costs</td><td>medium</td><td>lowest</td><td>highest</td></tr><tr><td>Burden on the caregiver</td><td>medium</td><td>lowest</td><td>highest</td></tr><tr><td>Applications</td><td>communication</td><td>communication</td><td>communication (binary)</td></tr></tbody></table></table-wrap><list list-type="bullet"><list-item><p>How difficult/easy was it for you to control the EOG/eye tracking/EEG(BCI) based system, on a scale from 0 to 10 (if 0&#x2009;=&#x2009;very difficult and 10&#x2009;=&#x2009;very easy)?</p></list-item><list-item><p>How tiring was using the EOG/eye tracker/BCI for you, on a scale of 0 to 10 (if 0&#x2009;=&#x2009;not tiring at all and 10&#x2009;=&#x2009;extremely tiring)</p></list-item><list-item><p>How long (hours/minutes) do you think you would be able to use it before you would need a longer break?</p></list-item><list-item><p>Given that your current method of communication still works, would you consider using EOG/eye tracking/BCI as an additional communication method?</p><p>If no, which are the obstacles of use?</p><p>If yes, what would be the most important improvement?</p></list-item><list-item><p>Would you consider using the EOG/eye tracking/EEG(BCI) based system if your current AT system was no longer working?</p><p>If no, which are the obstacles of use?</p><p>If yes, what would be the most important improvement?</p></list-item></list></p>
      </sec>
    </sec>
    <sec id="Sec11" sec-type="results">
      <title>Results</title>
      <sec id="Sec12">
        <title>Performance</title>
        <p>The participant was able to control the EOG, eye tracking and BCI based system. Figure&#xA0;<xref rid="Fig2" ref-type="fig">2</xref> shows the performance comparison for all tested systems across all sessions.<fig id="Fig2"><label>Fig. 2</label><caption><p>Online selection accuracies for all tested systems and sessions. The mean accuracies for the EOG based system were calculated by weighting the depicted accuracies by the number of selections made. Starting from day 2, stimuli were presented in alphabetical order for the EOG based system and in session 3.2 and 3.3 two sequences were used instead of one. Please refer to Table&#xA0;<xref rid="Tab1" ref-type="table">1</xref> for a detailed listing of the applied parameters and selections made in each session</p></caption><graphic xlink:href="12984_2015_71_Fig2_HTML" id="MO2"/></fig></p>
        <p>With the EOG based system the user reached above 70&#xA0;% accuracy with the two choice paradigm on the first day, but had difficulties with five selections on the same day. When we presented the 5 letters in alphabetical order on day two, he reached an accuracy of 79&#xA0;%. On day 3, we introduced the two step procedure that theoretically allowed him to choose any letter of the latin alphabet except the Z. He reached 100&#xA0;% accuracy in session 3.2 in which we asked him to spell two 3 letter words (12 selections in total), but two sequences (stimulus repetitions) were needed. In session 3.3 the user tried to spell a word of his choice, but stopped after the third attempt (fourth selection), because he could not concentrate on the target sounds necessary to select the desired letters (unable to recall the position in the spelling tree).</p>
        <p>With the eye tracking based system all selections were classified correctly. However, placing the eye tracker in front of the user in a way that the system could recognize the reflections of the light source on his cornea and in his pupil was difficult. Set-up and calibration took about 20&#xA0;min. The user had difficulties looking in a particular direction; therefore we facilitated the task for the first 12 selections, by holding a pen above the eye tracker within the designated target half to help him fixate. For the remaining selections, in which the user could choose the side he wanted to look to, we asked him to only make small eye movements to either of the sides. The system still identified all intended selections correctly.</p>
        <p>With the auditory BCI, the study participant reached accuracies above 70&#xA0;% on all three days of testing. To reach this threshold, 10 or 20 sequences were needed during online selections. Exemplary EOG and EEG traces are provided as Additional files <xref rid="MOESM1" ref-type="media">1</xref> and <xref rid="MOESM2" ref-type="media">2</xref>.</p>
      </sec>
      <sec id="Sec13">
        <title>User feedback</title>
        <p>Figure&#xA0;<xref rid="Fig3" ref-type="fig">3</xref> depicts a comparison of the user ratings regarding the ease of use and the operator fatigue for all systems. The user indicated that he would not consider any of the tested systems as an additional method of communication, although he estimated that he could use the EOG for about 2&#xA0;h, the eye tracker for 4&#xA0;h and the BCI for about 1&#xA0;h before he would need a longer break. The obstacles of use for the EOG system were the strong eye movements required for the system to work and he stated that it was unlikely that the system would be able to detect movements that could not also be detected by the caregiver. Although the eye tracking system would allow him to communicate independent of the caregiver, he expressed a clear preference for the partner scanning based approach. The only system that he would consider using if he was no longer able to control his current method, was the BCI. He did not think that the other systems could help him in that case. He considered the ease of use of the BCI as the highest among the tested systems, but also rated it as the most tiring.<fig id="Fig3"><label>Fig. 3</label><caption><p>User ratings (VAS) of fatigue and ease of use for the tested systems</p></caption><graphic xlink:href="12984_2015_71_Fig3_HTML" id="MO3"/></fig></p>
      </sec>
    </sec>
    <sec id="Sec14" sec-type="discussion">
      <title>Discussion</title>
      <p>The study demonstrated that a person in the locked-in state can gain control over an EOG based system, an eye tracker and an auditory BCI for binary communication with satisfactory accuracies (&gt;70&#xA0;% correct). Nevertheless, the participant expressed a preference for his low-tech method of communication over the tested systems and did not consider using them as an additional method. The only system that he would consider as a communication aid is the BCI, in case the partner assisted scanning approach (with a letter board based on eye movements) was no longer working.</p>
      <p>The focus of this case study was on testing possible input signals that could be implemented to control AAC devices, rather than providing ready-to-use communication applications. Due to the study design with only one participant, it is important to stress that generalization to the general population of persons in LIS is not possible. The iterative process of tailoring the different technologies to the participant&#x2019;s needs prevented us from comparing them in terms of assistive efficacy. Therefore, we will discuss in detail the results obtained for the participant and only briefly address the potential of each system as AAC device with references to other studies.</p>
      <p>An overview of the main features of the tested systems is provided in Table&#xA0;<xref rid="Tab2" ref-type="table">2</xref>. In the following, we will discuss the results obtained with each system and considerations for future work.</p>
      <sec id="Sec15">
        <title>Electrooculogram</title>
        <p>The participant rated the ease of use of the EOG lowest. The main reason was the relatively strong horizontal eye movements required that were strenuous for the participant. While it was therefore not an option for the participant as an input signal for AAC, it could be a good option for some users. For instance, Kaufmann et al. [<xref ref-type="bibr" rid="CR8">8</xref>] showed that lifts of the eyelid of their study participant with LIS could reliably be classified and used for spelling of words after a short calibration phase.</p>
      </sec>
      <sec id="Sec16">
        <title>Eye tracking</title>
        <p>The participant of the current study rated the eye tracker as the least tiring of the tested systems. A similar result was obtained by Pasqualotto et al. [<xref ref-type="bibr" rid="CR23">23</xref>], who could show that the workload of an eye tracker was lower than that of a P300 BCI. Although an eye tracking based system would allow for communication independent of the caregivers or even control of technical devices (e.g., a TV or radio), the participant of our study did not consider it as an additional communication method. The participant had full-time care and with most caregivers and family members, who we met during the study, he had achieved a high speed of communication. This was mostly due to the familiarity of the caregivers and the participant. Hence, the communication partners could suggest letters or words based on their personal knowledge about the participant and based on the context or topic of the conversation. They could also react flexibly to his ability to move the eyes and adapt the partner scanning approach accordingly. This level of flexibility and efficiency would be difficult to reach with an eye-tracking based system. Another reason for his preference of the low-tech method may have been that during the partner assisted communication method there is a direct interaction between the user and the interlocutor.</p>
        <p>Despite the demands on the communication partners, the spontaneous face-to-face conversation mode was reported by caregivers to be the most frequent with persons with ALS in a study by Fried-Oken et al. [<xref ref-type="bibr" rid="CR42">42</xref>]. Similarly, in a study by Spataro et al. [<xref ref-type="bibr" rid="CR6">6</xref>] half of the study participants, who had an eye tracking system available, communicated regularly with a letter board. Further, there is evidence that these low-tech methods are predominant for persons with most severe levels of disability (e.g., persons in the locked-in state) [<xref ref-type="bibr" rid="CR5">5</xref>, <xref ref-type="bibr" rid="CR6">6</xref>, <xref ref-type="bibr" rid="CR43">43</xref>]. Nevertheless<italic>,</italic> for those persons with no residual motor control except eye movements, it is often the only (commercial) method to gain access to AAC. Studies including persons with late-stage ALS could show an improvement in the quality of life for persons using an eye tracking system, a high satisfaction with the system for most participants and a reduced burden on caregivers [<xref ref-type="bibr" rid="CR6">6</xref>, <xref ref-type="bibr" rid="CR44">44</xref>&#x2013;<xref ref-type="bibr" rid="CR46">46</xref>]. Shortcomings frequently reported are difficulties operating under a range of changing light and postural conditions, determining the optimal dwell time and technical support required [<xref ref-type="bibr" rid="CR5">5</xref>, <xref ref-type="bibr" rid="CR43">43</xref>, <xref ref-type="bibr" rid="CR47">47</xref>, <xref ref-type="bibr" rid="CR48">48</xref>].</p>
      </sec>
      <sec id="Sec17">
        <title>Brain-computer interfaces</title>
        <p>In this study we tested an auditory ERP-BCI with simple to understand instructions (&#x201C;please listen to the high/low pitched tone&#x201D;) that would allow for binary communication [<xref ref-type="bibr" rid="CR26">26</xref>]. The participant achieved a satisfactory level of control. He rated the ease of use as the highest of the tested systems, but also described it as the most tiring. These ratings suggest that the ease of use was rated as high since no muscle movements were required, but that the attentional demands were substantially higher for the BCI compared to the other systems. Due to the deafness of his right ear, we could not test a streaming approach as suggested by Hill &amp; Sch&#xF6;lkopf [<xref ref-type="bibr" rid="CR29">29</xref>, <xref ref-type="bibr" rid="CR30">30</xref>] that requires dichotic listening. For a person with intact hearing, a streaming approach could substantially speed up the selection rate. Another improvement could be to replace the beep sounds by more natural sounds, such as recordings of yes or no [<xref ref-type="bibr" rid="CR28">28</xref>, <xref ref-type="bibr" rid="CR31">31</xref>].</p>
        <p>For persons with long attention spans and sufficient cognitive abilities, auditory multi-class BCIs could provide spelling solutions (e.g., [<xref ref-type="bibr" rid="CR33">33</xref>&#x2013;<xref ref-type="bibr" rid="CR38">38</xref>]). With training, a satisfactory level of control could be achieved [<xref ref-type="bibr" rid="CR49">49</xref>, <xref ref-type="bibr" rid="CR50">50</xref>]. Tactile ERP-BCIs could be an option for persons who are unable to control auditory BCIs [<xref ref-type="bibr" rid="CR8">8</xref>, <xref ref-type="bibr" rid="CR24">24</xref>].</p>
        <p>In case all voluntary muscular activity is lost, i.e., in the complete locked-in state, communication based on recordings of brain-activity might still be possible. Brain-computer interfaces enabled severely paralyzed persons, even users in the locked-in state, to communicate [<xref ref-type="bibr" rid="CR17">17</xref>, <xref ref-type="bibr" rid="CR18">18</xref>, <xref ref-type="bibr" rid="CR51">51</xref>, <xref ref-type="bibr" rid="CR52">52</xref>]. However, only few reports on communication attempts with persons in the complete locked-in state exist [<xref ref-type="bibr" rid="CR53">53</xref>, <xref ref-type="bibr" rid="CR54">54</xref>]; see [<xref ref-type="bibr" rid="CR17">17</xref>] for an overview) and only one reported significantly above chancel level performance [<xref ref-type="bibr" rid="CR55">55</xref>].</p>
      </sec>
      <sec id="Sec18">
        <title>Considerations for future work</title>
        <p>Brain-computer interfaces could prove valuable for participants in situations when the preferred method is not operable due to muscular fatigue among others. For these situations, a robust and simple to operate BCI is needed, particularly for those users, who usually rely on low-tech partner assisted communication such as the participant of this study. For persons, who use more advanced AT such as an eye tracker, a complementary or hybrid approach could ease the transition to a purely BCI based AT if control over all muscles is lost (i.e., in CLIS caused by a neurodegenerative disease). Recently hybrid BCIs (hBCIs) were proposed that consist of a combination of one or more conventional AT input devices or biosignals and at least one BCI channel (for a review of the state-of-the-art see [<xref ref-type="bibr" rid="CR56">56</xref>]). Although case studies demonstrated the feasibility of long-term independent home use of BCIs, [<xref ref-type="bibr" rid="CR21">21</xref>, <xref ref-type="bibr" rid="CR22">22</xref>, <xref ref-type="bibr" rid="CR57">57</xref>], usability has to be improved before BCIs be considered as assistive technology for a larger group of persons. To achieve this goal, it is important to engage the potential end-users during all steps of the development process from the definition of user requirements to the implementation and the iterative testing of prototypes [<xref ref-type="bibr" rid="CR58">58</xref>]. Measures to operationalize usability for the evaluation of BCIs were proposed by [<xref ref-type="bibr" rid="CR21">21</xref>, <xref ref-type="bibr" rid="CR59">59</xref>, <xref ref-type="bibr" rid="CR60">60</xref>] among others.</p>
      </sec>
    </sec>
    <sec id="Sec19" sec-type="conclusion">
      <title>Conclusions</title>
      <p>We demonstrated that a person in the locked-in state was able to gain control over an EOG, an eye tracker and an auditory brain-computer interface. Due to the end user&#x2019;s and his caregivers&#x2019; proficiency in using a low-tech communication method based on residual eye movements, he did not consider using the tested systems as an access method to AAC. He would consider using a BCI once control over his eye muscles will be no longer possible.</p>
      <p>BCIs can extend the range of available access methods and the combination with existing assistive technology should be considered. A user-centered design approach in the development of these systems will increase the likelihood that they will be used as assistive technology in daily life. For persons with severe paralysis, who could benefit from a BCI (immediately or in the future), solutions tailored to the users&#x2019; individual needs are required and thus, the engagement of targeted end-users in all steps of the development process is needed as requested by the user-centered design [<xref ref-type="bibr" rid="CR58">58</xref>].</p>
    </sec>
  </body>
  <back>
    <app-group>
      <app id="App1">
        <sec id="Sec20">
          <title>Additional files</title>
          <p>
            <media position="anchor" xlink:href="12984_2015_71_MOESM1_ESM.pdf" id="MOESM1">
              <label>Additional file 1: Figure S1.</label>
              <caption>
                <p>Average EOG traces for the training run on day 3. The task of the participant consisted of looking to the right and back to the center in response to the target letter. (PDF 18&#xA0;kb)</p>
              </caption>
            </media>
            <media position="anchor" xlink:href="12984_2015_71_MOESM2_ESM.pdf" id="MOESM2">
              <label>Additional file 2: Figure S2.</label>
              <caption>
                <p>Averaged ERP waveforms at Cz; and scalp plot of differential ERP activity (targets minus non-targets) at the peak latency of the P300 for session 2.1. (PDF 88&#xA0;kb)</p>
              </caption>
            </media>
          </p>
        </sec>
      </app>
    </app-group>
    <fn-group>
      <fn>
        <p>
          <bold>Competing interests</bold>
        </p>
        <p>The authors declare that they have no competing interests.</p>
      </fn>
      <fn>
        <p>
          <bold>Authors&#x2019; contributions</bold>
        </p>
        <p>IK and SH conducted the experiments. SH modified the software for stimulus presentation and control with the eye tracker. IK analyzed the data and drafted the manuscript. SH and AK helped to draft the manuscript. IK, SH and AK conceived of the study, IK and SH participated in its design and coordination. All authors read and approved the final manuscript.</p>
      </fn>
    </fn-group>
    <ack>
      <title>Acknowledgements</title>
      <p>The authors would like to thank the participant and his family for their support and patience during the evaluation of the systems and Tobias Kaufmann for his help during the planning of this study. The study was funded by the European ICT Program Project FP7-288566. This publication was funded by the German Research Foundation (DFG) and the University of W&#xFC;rzburg in the funding program Open Access Publishing. This paper only reflects the authors&#x2019; views and funding agencies are not liable for any use that may be made of the information contained herein. SH received funding as International Research Fellow of the Japan Society for the Promotion of Science and the Alexander von Humboldt Foundation.</p>
    </ack>
    <ref-list id="Bib1">
      <title>References</title>
      <ref id="CR1">
        <label>1.</label>
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Posner</surname>
              <given-names>JB</given-names>
            </name>
            <name>
              <surname>Saper</surname>
              <given-names>CB</given-names>
            </name>
            <name>
              <surname>Schiff</surname>
              <given-names>N</given-names>
            </name>
            <name>
              <surname>Plum</surname>
              <given-names>F</given-names>
            </name>
          </person-group>
          <source>Plum and Posner&#x2019;s Diagnosis of Stupor and Coma</source>
          <year>2007</year>
          <edition>4</edition>
          <publisher-loc>New York</publisher-loc>
          <publisher-name>Oxford University Press</publisher-name>
        </element-citation>
      </ref>
      <ref id="CR2">
        <label>2.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <collab>American Congress of Rehabilitation Medicine</collab>
          </person-group>
          <article-title>Recommendations for use of uniform nomenclature pertinent to patients with severe alterations in consciousness</article-title>
          <source>Arch Phys Med Rehabil</source>
          <year>1995</year>
          <volume>76</volume>
          <issue>2</issue>
          <fpage>205</fpage>
          <lpage>9</lpage>
          <pub-id pub-id-type="doi">10.1016/S0003-9993(95)80031-X</pub-id>
          <pub-id pub-id-type="pmid">7848080</pub-id>
        </element-citation>
      </ref>
      <ref id="CR3">
        <label>3.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Bauer</surname>
              <given-names>G</given-names>
            </name>
            <name>
              <surname>Gerstenbrand</surname>
              <given-names>F</given-names>
            </name>
            <name>
              <surname>Rumpl</surname>
              <given-names>E</given-names>
            </name>
          </person-group>
          <article-title>Varieties of the locked-in syndrome</article-title>
          <source>J Neurol</source>
          <year>1979</year>
          <volume>221</volume>
          <issue>2</issue>
          <fpage>77</fpage>
          <lpage>91</lpage>
          <pub-id pub-id-type="doi">10.1007/BF00313105</pub-id>
          <pub-id pub-id-type="pmid">92545</pub-id>
        </element-citation>
      </ref>
      <ref id="CR4">
        <label>4.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Birbaumer</surname>
              <given-names>N</given-names>
            </name>
          </person-group>
          <article-title>Breaking the silence: Brain-computer interfaces (BCI) for communication and motor control</article-title>
          <source>Psychophysiology</source>
          <year>2006</year>
          <volume>43</volume>
          <issue>6</issue>
          <fpage>517</fpage>
          <lpage>32</lpage>
          <pub-id pub-id-type="doi">10.1111/j.1469-8986.2006.00456.x</pub-id>
          <pub-id pub-id-type="pmid">17076808</pub-id>
        </element-citation>
      </ref>
      <ref id="CR5">
        <label>5.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Beukelman</surname>
              <given-names>DR</given-names>
            </name>
            <name>
              <surname>Fager</surname>
              <given-names>S</given-names>
            </name>
            <name>
              <surname>Ball</surname>
              <given-names>L</given-names>
            </name>
            <name>
              <surname>Dietz</surname>
              <given-names>A</given-names>
            </name>
          </person-group>
          <article-title>AAC for adults with acquired neurological conditions: A review</article-title>
          <source>Augment Altern Commun</source>
          <year>2007</year>
          <volume>23</volume>
          <issue>3</issue>
          <fpage>230</fpage>
          <lpage>42</lpage>
          <pub-id pub-id-type="doi">10.1080/07434610701553668</pub-id>
          <pub-id pub-id-type="pmid">17701742</pub-id>
        </element-citation>
      </ref>
      <ref id="CR6">
        <label>6.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Spataro</surname>
              <given-names>R</given-names>
            </name>
            <name>
              <surname>Ciriacono</surname>
              <given-names>M</given-names>
            </name>
            <name>
              <surname>Manno</surname>
              <given-names>C</given-names>
            </name>
            <name>
              <surname>La Bella</surname>
              <given-names>V</given-names>
            </name>
          </person-group>
          <article-title>The eye-tracking computer device for communication in amyotrophic lateral sclerosis</article-title>
          <source>Acta Neurol Scand</source>
          <year>2014</year>
          <volume>130</volume>
          <issue>1</issue>
          <fpage>40</fpage>
          <lpage>5</lpage>
          <pub-id pub-id-type="doi">10.1111/ane.12214</pub-id>
          <pub-id pub-id-type="pmid">24350578</pub-id>
        </element-citation>
      </ref>
      <ref id="CR7">
        <label>7.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Deng</surname>
              <given-names>LY</given-names>
            </name>
            <name>
              <surname>Hsu</surname>
              <given-names>C-L</given-names>
            </name>
            <name>
              <surname>Lin</surname>
              <given-names>T-C</given-names>
            </name>
            <name>
              <surname>Tuan</surname>
              <given-names>J-S</given-names>
            </name>
            <name>
              <surname>Chang</surname>
              <given-names>S-M</given-names>
            </name>
          </person-group>
          <article-title>EOG-based Human&#x2013;Computer Interface system development</article-title>
          <source>Expert Systems Applications</source>
          <year>2010</year>
          <volume>37</volume>
          <issue>4</issue>
          <fpage>3337</fpage>
          <lpage>43</lpage>
          <pub-id pub-id-type="doi">10.1016/j.eswa.2009.10.017</pub-id>
        </element-citation>
      </ref>
      <ref id="CR8">
        <label>8.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Kaufmann</surname>
              <given-names>T</given-names>
            </name>
            <name>
              <surname>Holz</surname>
              <given-names>E</given-names>
            </name>
            <name>
              <surname>K&#xFC;bler</surname>
              <given-names>A</given-names>
            </name>
          </person-group>
          <article-title>Comparison of tactile, auditory, and visual modality for brain-computer interface use: a case study with a patient in the locked-in state</article-title>
          <source>Front Neurosci</source>
          <year>2013</year>
          <volume>7</volume>
          <fpage>129</fpage>
          <pub-id pub-id-type="doi">10.3389/fnins.2013.00129</pub-id>
          <pub-id pub-id-type="pmid">23898236</pub-id>
        </element-citation>
      </ref>
      <ref id="CR9">
        <label>9.</label>
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Tomita</surname>
              <given-names>Y</given-names>
            </name>
            <name>
              <surname>Igarashi</surname>
              <given-names>Y</given-names>
            </name>
            <name>
              <surname>Honda</surname>
              <given-names>S</given-names>
            </name>
            <name>
              <surname>Matsuo</surname>
              <given-names>N</given-names>
            </name>
          </person-group>
          <source>Electro-oculography mouse for amyotrophic lateral sclerosis patients</source>
          <year>1996</year>
          <fpage>1780</fpage>
          <lpage>1</lpage>
        </element-citation>
      </ref>
      <ref id="CR10">
        <label>10.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>K&#xFC;bler</surname>
              <given-names>A</given-names>
            </name>
            <name>
              <surname>Kotchoubey</surname>
              <given-names>B</given-names>
            </name>
            <name>
              <surname>Kaiser</surname>
              <given-names>J</given-names>
            </name>
            <name>
              <surname>Wolpaw</surname>
              <given-names>JR</given-names>
            </name>
            <name>
              <surname>Birbaumer</surname>
              <given-names>N</given-names>
            </name>
          </person-group>
          <article-title>Brain&#x2013;computer communication: Unlocking the locked in</article-title>
          <source>Psychol Bull</source>
          <year>2001</year>
          <volume>127</volume>
          <issue>3</issue>
          <fpage>358</fpage>
          <lpage>75</lpage>
          <pub-id pub-id-type="doi">10.1037/0033-2909.127.3.358</pub-id>
          <pub-id pub-id-type="pmid">11393301</pub-id>
        </element-citation>
      </ref>
      <ref id="CR11">
        <label>11.</label>
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Wolpaw</surname>
              <given-names>JR</given-names>
            </name>
            <name>
              <surname>Wolpaw</surname>
              <given-names>EW</given-names>
            </name>
          </person-group>
          <source>Brain-Computer Interfaces: Principles and Practice. Oxford</source>
          <year>2012</year>
          <publisher-loc>New York</publisher-loc>
          <publisher-name>Oxford Univ Pr</publisher-name>
        </element-citation>
      </ref>
      <ref id="CR12">
        <label>12.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Farwell</surname>
              <given-names>LA</given-names>
            </name>
            <name>
              <surname>Donchin</surname>
              <given-names>E</given-names>
            </name>
          </person-group>
          <article-title>Talking off the top of your head: toward a mental prosthesis utilizing event-related brain potentials</article-title>
          <source>Electroencephalogr Clin Neurophysiol</source>
          <year>1988</year>
          <volume>70</volume>
          <issue>6</issue>
          <fpage>510</fpage>
          <lpage>23</lpage>
          <pub-id pub-id-type="doi">10.1016/0013-4694(88)90149-6</pub-id>
          <pub-id pub-id-type="pmid">2461285</pub-id>
        </element-citation>
      </ref>
      <ref id="CR13">
        <label>13.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Kleih</surname>
              <given-names>SC</given-names>
            </name>
            <name>
              <surname>Nijboer</surname>
              <given-names>F</given-names>
            </name>
            <name>
              <surname>Halder</surname>
              <given-names>S</given-names>
            </name>
            <name>
              <surname>K&#xFC;bler</surname>
              <given-names>A</given-names>
            </name>
          </person-group>
          <article-title>Motivation modulates the P300 amplitude during brain&#x2013;computer interface use</article-title>
          <source>Clin Neurophysiol</source>
          <year>2010</year>
          <volume>121</volume>
          <issue>7</issue>
          <fpage>1023</fpage>
          <lpage>31</lpage>
          <pub-id pub-id-type="doi">10.1016/j.clinph.2010.01.034</pub-id>
          <pub-id pub-id-type="pmid">20188627</pub-id>
        </element-citation>
      </ref>
      <ref id="CR14">
        <label>14.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Mak</surname>
              <given-names>JN</given-names>
            </name>
            <name>
              <surname>Arbel</surname>
              <given-names>Y</given-names>
            </name>
            <name>
              <surname>Minett</surname>
              <given-names>JW</given-names>
            </name>
            <name>
              <surname>McCane</surname>
              <given-names>LM</given-names>
            </name>
            <name>
              <surname>Yuksel</surname>
              <given-names>B</given-names>
            </name>
            <name>
              <surname>Ryan</surname>
              <given-names>D</given-names>
            </name>
            <etal/>
          </person-group>
          <article-title>Optimizing the P300- based brain&#x2013;computer interface: current status, limitations and future directions</article-title>
          <source>J Neural Eng</source>
          <year>2011</year>
          <volume>8</volume>
          <issue>2</issue>
          <fpage>025003</fpage>
          <pub-id pub-id-type="doi">10.1088/1741-2560/8/2/025003</pub-id>
          <pub-id pub-id-type="pmid">21436525</pub-id>
        </element-citation>
      </ref>
      <ref id="CR15">
        <label>15.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Polich</surname>
              <given-names>J</given-names>
            </name>
          </person-group>
          <article-title>Updating P300: An integrative theory of P3a and P3b</article-title>
          <source>Clin Neurophysiol</source>
          <year>2007</year>
          <volume>118</volume>
          <issue>10</issue>
          <fpage>2128</fpage>
          <lpage>48</lpage>
          <pub-id pub-id-type="doi">10.1016/j.clinph.2007.04.019</pub-id>
          <pub-id pub-id-type="pmid">17573239</pub-id>
        </element-citation>
      </ref>
      <ref id="CR16">
        <label>16.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Guger</surname>
              <given-names>C</given-names>
            </name>
            <name>
              <surname>Daban</surname>
              <given-names>S</given-names>
            </name>
            <name>
              <surname>Sellers</surname>
              <given-names>E</given-names>
            </name>
            <name>
              <surname>Holzner</surname>
              <given-names>C</given-names>
            </name>
            <name>
              <surname>Krausz</surname>
              <given-names>G</given-names>
            </name>
            <name>
              <surname>Carabalona</surname>
              <given-names>R</given-names>
            </name>
            <etal/>
          </person-group>
          <article-title>How many people are able to control a P300-based brain&#x2013;computer interface (BCI)?</article-title>
          <source>Neurosci Lett</source>
          <year>2009</year>
          <volume>462</volume>
          <issue>1</issue>
          <fpage>94</fpage>
          <lpage>8</lpage>
          <pub-id pub-id-type="doi">10.1016/j.neulet.2009.06.045</pub-id>
          <pub-id pub-id-type="pmid">19545601</pub-id>
        </element-citation>
      </ref>
      <ref id="CR17">
        <label>17.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>K&#xFC;bler</surname>
              <given-names>A</given-names>
            </name>
            <name>
              <surname>Birbaumer</surname>
              <given-names>N</given-names>
            </name>
          </person-group>
          <article-title>Brain&#x2013;computer interfaces and communication in paralysis: Extinction of goal directed thinking in completely paralysed patients?</article-title>
          <source>Clin Neurophysiol</source>
          <year>2008</year>
          <volume>119</volume>
          <issue>11</issue>
          <fpage>2658</fpage>
          <lpage>66</lpage>
          <pub-id pub-id-type="doi">10.1016/j.clinph.2008.06.019</pub-id>
          <pub-id pub-id-type="pmid">18824406</pub-id>
        </element-citation>
      </ref>
      <ref id="CR18">
        <label>18.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Nijboer</surname>
              <given-names>SEW</given-names>
            </name>
            <name>
              <surname>Mellinger</surname>
              <given-names>J</given-names>
            </name>
            <name>
              <surname>Jordan</surname>
              <given-names>MA</given-names>
            </name>
            <name>
              <surname>Matuz</surname>
              <given-names>T</given-names>
            </name>
            <name>
              <surname>Furdea</surname>
              <given-names>A</given-names>
            </name>
            <etal/>
          </person-group>
          <article-title>A P300-based brain-computer interface for people with amyotrophic lateral sclerosis</article-title>
          <source>Clin Neurophysiol</source>
          <year>2008</year>
          <volume>119</volume>
          <issue>8</issue>
          <fpage>1909</fpage>
          <lpage>16</lpage>
          <pub-id pub-id-type="doi">10.1016/j.clinph.2008.03.034</pub-id>
          <pub-id pub-id-type="pmid">18571984</pub-id>
        </element-citation>
      </ref>
      <ref id="CR19">
        <label>19.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Townsend</surname>
              <given-names>G</given-names>
            </name>
            <name>
              <surname>LaPallo</surname>
              <given-names>BK</given-names>
            </name>
            <name>
              <surname>Boulay</surname>
              <given-names>CB</given-names>
            </name>
            <name>
              <surname>Krusienski</surname>
              <given-names>DJ</given-names>
            </name>
            <name>
              <surname>Frye</surname>
              <given-names>GE</given-names>
            </name>
            <name>
              <surname>Hauser</surname>
              <given-names>CK</given-names>
            </name>
            <etal/>
          </person-group>
          <article-title>A novel P300-based brain&#x2013;computer interface stimulus presentation paradigm: Moving beyond rows and columns</article-title>
          <source>Clin Neurophysiol</source>
          <year>2010</year>
          <volume>121</volume>
          <issue>7</issue>
          <fpage>1109</fpage>
          <lpage>20</lpage>
          <pub-id pub-id-type="doi">10.1016/j.clinph.2010.01.030</pub-id>
          <pub-id pub-id-type="pmid">20347387</pub-id>
        </element-citation>
      </ref>
      <ref id="CR20">
        <label>20.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Halder</surname>
              <given-names>S</given-names>
            </name>
            <name>
              <surname>Pinegger</surname>
              <given-names>A</given-names>
            </name>
            <name>
              <surname>K&#xE4;thner</surname>
              <given-names>I</given-names>
            </name>
            <name>
              <surname>Wriessnegger</surname>
              <given-names>SC</given-names>
            </name>
            <name>
              <surname>Faller</surname>
              <given-names>J</given-names>
            </name>
            <name>
              <surname>Pires Antunes</surname>
              <given-names>JB</given-names>
            </name>
            <etal/>
          </person-group>
          <article-title>Brain-controlled applications using dynamic P300 speller matrices</article-title>
          <source>Artif Intell Med</source>
          <year>2015</year>
          <volume>63</volume>
          <issue>1</issue>
          <fpage>7</fpage>
          <lpage>17</lpage>
          <pub-id pub-id-type="doi">10.1016/j.artmed.2014.12.001</pub-id>
          <pub-id pub-id-type="pmid">25533310</pub-id>
        </element-citation>
      </ref>
      <ref id="CR21">
        <label>21.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Holz</surname>
              <given-names>EM</given-names>
            </name>
            <name>
              <surname>Botrel</surname>
              <given-names>L</given-names>
            </name>
            <name>
              <surname>Kaufmann</surname>
              <given-names>T</given-names>
            </name>
            <name>
              <surname>K&#xFC;bler</surname>
              <given-names>A</given-names>
            </name>
          </person-group>
          <article-title>Long-Term Independent Brain-Computer Interface Home Use Improves Quality of Life of a Patient in the Locked-In State: A Case Study</article-title>
          <source>Arch Phys Med Rehabil</source>
          <year>2015</year>
          <volume>96</volume>
          <issue>3, Supplement</issue>
          <fpage>16</fpage>
          <lpage>26</lpage>
          <pub-id pub-id-type="doi">10.1016/j.apmr.2014.03.035</pub-id>
        </element-citation>
      </ref>
      <ref id="CR22">
        <label>22.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Sellers</surname>
              <given-names>EW</given-names>
            </name>
            <name>
              <surname>Vaughan</surname>
              <given-names>TM</given-names>
            </name>
            <name>
              <surname>Wolpaw</surname>
              <given-names>JR</given-names>
            </name>
          </person-group>
          <article-title>A brain-computer interface for long-term independent home use</article-title>
          <source>Amyotroph Lateral Scler</source>
          <year>2010</year>
          <volume>11</volume>
          <issue>5</issue>
          <fpage>449</fpage>
          <lpage>55</lpage>
          <pub-id pub-id-type="doi">10.3109/17482961003777470</pub-id>
          <pub-id pub-id-type="pmid">20583947</pub-id>
        </element-citation>
      </ref>
      <ref id="CR23">
        <label>23.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Pasqualotto</surname>
              <given-names>E</given-names>
            </name>
            <name>
              <surname>Matuz</surname>
              <given-names>T</given-names>
            </name>
            <name>
              <surname>Federici</surname>
              <given-names>S</given-names>
            </name>
            <name>
              <surname>Ruf</surname>
              <given-names>CA</given-names>
            </name>
            <name>
              <surname>Bartl</surname>
              <given-names>M</given-names>
            </name>
            <name>
              <surname>Belardinelli</surname>
              <given-names>MO</given-names>
            </name>
            <etal/>
          </person-group>
          <article-title>Usability and Workload of Access Technology for People With Severe Motor Impairment A Comparison of Brain-Computer Interfacing and Eye Tracking</article-title>
          <source>Neurorehabil Neural Repair</source>
          <year>2015</year>
          <pub-id pub-id-type="pmid">25753951</pub-id>
        </element-citation>
      </ref>
      <ref id="CR24">
        <label>24.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Riccio</surname>
              <given-names>A</given-names>
            </name>
            <name>
              <surname>Mattia</surname>
              <given-names>D</given-names>
            </name>
            <name>
              <surname>Simione</surname>
              <given-names>L</given-names>
            </name>
            <name>
              <surname>Olivetti</surname>
              <given-names>M</given-names>
            </name>
            <name>
              <surname>Cincotti</surname>
              <given-names>F</given-names>
            </name>
          </person-group>
          <article-title>Eye-gaze independent EEG-based brain-computer interfaces for communication</article-title>
          <source>J Neural Eng</source>
          <year>2012</year>
          <volume>9</volume>
          <issue>4</issue>
          <fpage>045001</fpage>
          <pub-id pub-id-type="doi">10.1088/1741-2560/9/4/045001</pub-id>
          <pub-id pub-id-type="pmid">22831893</pub-id>
        </element-citation>
      </ref>
      <ref id="CR25">
        <label>25.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>De Vos</surname>
              <given-names>M</given-names>
            </name>
            <name>
              <surname>Gandras</surname>
              <given-names>K</given-names>
            </name>
            <name>
              <surname>Debener</surname>
              <given-names>S</given-names>
            </name>
          </person-group>
          <article-title>Towards a truly mobile auditory brain&#x2013;computer interface: Exploring the P300 to take away</article-title>
          <source>Int J Psychophysiol</source>
          <year>2014</year>
          <volume>91</volume>
          <issue>1</issue>
          <fpage>46</fpage>
          <lpage>53</lpage>
          <pub-id pub-id-type="doi">10.1016/j.ijpsycho.2013.08.010</pub-id>
          <pub-id pub-id-type="pmid">23994208</pub-id>
        </element-citation>
      </ref>
      <ref id="CR26">
        <label>26.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Halder</surname>
              <given-names>S</given-names>
            </name>
            <name>
              <surname>Rea</surname>
              <given-names>M</given-names>
            </name>
            <name>
              <surname>Andreoni</surname>
              <given-names>R</given-names>
            </name>
            <name>
              <surname>Nijboer</surname>
              <given-names>F</given-names>
            </name>
            <name>
              <surname>Hammer</surname>
              <given-names>EM</given-names>
            </name>
            <name>
              <surname>Kleih</surname>
              <given-names>SC</given-names>
            </name>
            <etal/>
          </person-group>
          <article-title>An auditory oddball brain&#x2013;computer interface for binary choices</article-title>
          <source>Clin Neurophysiol</source>
          <year>2010</year>
          <volume>121</volume>
          <issue>4</issue>
          <fpage>516</fpage>
          <lpage>23</lpage>
          <pub-id pub-id-type="doi">10.1016/j.clinph.2009.11.087</pub-id>
          <pub-id pub-id-type="pmid">20093075</pub-id>
        </element-citation>
      </ref>
      <ref id="CR27">
        <label>27.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Pokorny</surname>
              <given-names>C</given-names>
            </name>
            <name>
              <surname>Klobassa</surname>
              <given-names>DS</given-names>
            </name>
            <name>
              <surname>Pichler</surname>
              <given-names>G</given-names>
            </name>
            <name>
              <surname>Erlbeck</surname>
              <given-names>H</given-names>
            </name>
            <name>
              <surname>Real</surname>
              <given-names>RGL</given-names>
            </name>
            <name>
              <surname>K&#xFC;bler</surname>
              <given-names>A</given-names>
            </name>
            <etal/>
          </person-group>
          <article-title>The auditory P300-based single-switch brain&#x2013;computer interface: Paradigm transition from healthy subjects to minimally conscious patients</article-title>
          <source>Artif Intell Med</source>
          <year>2013</year>
          <volume>59</volume>
          <issue>2</issue>
          <fpage>81</fpage>
          <lpage>90</lpage>
          <pub-id pub-id-type="doi">10.1016/j.artmed.2013.07.003</pub-id>
          <pub-id pub-id-type="pmid">24076342</pub-id>
        </element-citation>
      </ref>
      <ref id="CR28">
        <label>28.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Sellers</surname>
              <given-names>EW</given-names>
            </name>
            <name>
              <surname>Donchin</surname>
              <given-names>E</given-names>
            </name>
          </person-group>
          <article-title>A P300-based brain&#x2013;computer interface: Initial tests by ALS patients</article-title>
          <source>Clin Neurophysiol</source>
          <year>2006</year>
          <volume>117</volume>
          <issue>3</issue>
          <fpage>538</fpage>
          <lpage>48</lpage>
          <pub-id pub-id-type="doi">10.1016/j.clinph.2005.06.027</pub-id>
          <pub-id pub-id-type="pmid">16461003</pub-id>
        </element-citation>
      </ref>
      <ref id="CR29">
        <label>29.</label>
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Hill</surname>
              <given-names>NJ</given-names>
            </name>
            <name>
              <surname>Lal</surname>
              <given-names>TN</given-names>
            </name>
            <name>
              <surname>Bierig</surname>
              <given-names>K</given-names>
            </name>
            <name>
              <surname>Birbaumer</surname>
              <given-names>N</given-names>
            </name>
            <name>
              <surname>Sch&#xF6;lkopf</surname>
              <given-names>B</given-names>
            </name>
          </person-group>
          <person-group person-group-type="editor">
            <name>
              <surname>Saul</surname>
              <given-names>LK</given-names>
            </name>
            <name>
              <surname>Weiss</surname>
              <given-names>Y</given-names>
            </name>
            <name>
              <surname>Bottou</surname>
              <given-names>L</given-names>
            </name>
          </person-group>
          <article-title>An auditory paradigm for brain-computer interfaces</article-title>
          <source>Advances in Neural Information Processing Systems 17</source>
          <year>2005</year>
          <publisher-loc>Cambridge, MA</publisher-loc>
          <publisher-name>MIT Press</publisher-name>
          <fpage>569</fpage>
          <lpage>76</lpage>
        </element-citation>
      </ref>
      <ref id="CR30">
        <label>30.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Hill</surname>
              <given-names>NJ</given-names>
            </name>
            <name>
              <surname>Sch&#xF6;lkopf</surname>
              <given-names>B</given-names>
            </name>
          </person-group>
          <article-title>An online brain&#x2013;computer interface based on shifting attention to concurrent streams of auditory stimuli</article-title>
          <source>J Neural Eng</source>
          <year>2012</year>
          <volume>9</volume>
          <issue>2</issue>
          <fpage>026011</fpage>
          <pub-id pub-id-type="doi">10.1088/1741-2560/9/2/026011</pub-id>
          <pub-id pub-id-type="pmid">22333135</pub-id>
        </element-citation>
      </ref>
      <ref id="CR31">
        <label>31.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Hill</surname>
              <given-names>NJ</given-names>
            </name>
            <name>
              <surname>Ricci</surname>
              <given-names>E</given-names>
            </name>
            <name>
              <surname>Haider</surname>
              <given-names>S</given-names>
            </name>
            <name>
              <surname>McCane</surname>
              <given-names>LM</given-names>
            </name>
            <name>
              <surname>Heckman</surname>
              <given-names>S</given-names>
            </name>
            <name>
              <surname>Wolpaw</surname>
              <given-names>JR</given-names>
            </name>
            <etal/>
          </person-group>
          <article-title>A practical, intuitive brain&#x2013;computer interface for communicating &#x2018;yes&#x2019; or &#x2018;no&#x2019; by listening</article-title>
          <source>J Neural Eng</source>
          <year>2014</year>
          <volume>11</volume>
          <issue>3</issue>
          <fpage>035003</fpage>
          <pub-id pub-id-type="doi">10.1088/1741-2560/11/3/035003</pub-id>
          <pub-id pub-id-type="pmid">24838278</pub-id>
        </element-citation>
      </ref>
      <ref id="CR32">
        <label>32.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Furdea</surname>
              <given-names>A</given-names>
            </name>
            <name>
              <surname>Halder</surname>
              <given-names>S</given-names>
            </name>
            <name>
              <surname>Krusienski</surname>
              <given-names>DJ</given-names>
            </name>
            <name>
              <surname>Bross</surname>
              <given-names>D</given-names>
            </name>
            <name>
              <surname>Nijboer</surname>
              <given-names>F</given-names>
            </name>
            <name>
              <surname>Birbaumer</surname>
              <given-names>N</given-names>
            </name>
            <etal/>
          </person-group>
          <article-title>An auditory oddball (P300) spelling system for brain-computer interfaces</article-title>
          <source>Psychophysiology</source>
          <year>2009</year>
          <volume>46</volume>
          <issue>3</issue>
          <fpage>617</fpage>
          <lpage>25</lpage>
          <pub-id pub-id-type="doi">10.1111/j.1469-8986.2008.00783.x</pub-id>
          <pub-id pub-id-type="pmid">19170946</pub-id>
        </element-citation>
      </ref>
      <ref id="CR33">
        <label>33.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>H&#xF6;hne</surname>
              <given-names>J</given-names>
            </name>
            <name>
              <surname>Schreuder</surname>
              <given-names>M</given-names>
            </name>
            <name>
              <surname>Blankertz</surname>
              <given-names>B</given-names>
            </name>
            <name>
              <surname>Tangermann</surname>
              <given-names>M</given-names>
            </name>
          </person-group>
          <article-title>A novel 9-class auditory ERP paradigm driving a predictive text entry system</article-title>
          <source>Front Neurosci</source>
          <year>2011</year>
          <volume>5</volume>
          <fpage>99</fpage>
          <pub-id pub-id-type="doi">10.3389/fnins.2011.00099</pub-id>
          <pub-id pub-id-type="pmid">21909321</pub-id>
        </element-citation>
      </ref>
      <ref id="CR34">
        <label>34.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>H&#xF6;hne</surname>
              <given-names>J</given-names>
            </name>
            <name>
              <surname>Tangermann</surname>
              <given-names>M</given-names>
            </name>
          </person-group>
          <article-title>Towards User-Friendly Spelling with an Auditory Brain-Computer Interface: The CharStreamer Paradigm</article-title>
          <source>PLoS One</source>
          <year>2014</year>
          <volume>9</volume>
          <issue>6</issue>
          <fpage>e98322</fpage>
          <pub-id pub-id-type="doi">10.1371/journal.pone.0098322</pub-id>
          <pub-id pub-id-type="pmid">24886978</pub-id>
        </element-citation>
      </ref>
      <ref id="CR35">
        <label>35.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>K&#xE4;thner</surname>
              <given-names>I</given-names>
            </name>
            <name>
              <surname>Ruf</surname>
              <given-names>CA</given-names>
            </name>
            <name>
              <surname>Pasqualotto</surname>
              <given-names>E</given-names>
            </name>
            <name>
              <surname>Braun</surname>
              <given-names>C</given-names>
            </name>
            <name>
              <surname>Birbaumer</surname>
              <given-names>N</given-names>
            </name>
            <name>
              <surname>Halder</surname>
              <given-names>S</given-names>
            </name>
          </person-group>
          <article-title>A portable auditory P300 brain-computer interface with directional cues</article-title>
          <source>Clin Neurophysiol</source>
          <year>2013</year>
          <volume>124</volume>
          <issue>2</issue>
          <fpage>327</fpage>
          <lpage>38</lpage>
          <pub-id pub-id-type="doi">10.1016/j.clinph.2012.08.006</pub-id>
          <pub-id pub-id-type="pmid">22959257</pub-id>
        </element-citation>
      </ref>
      <ref id="CR36">
        <label>36.</label>
        <mixed-citation publication-type="other">Schreuder M, Blankertz B, Tangermann M. A New Auditory Multi-Class Brain-Computer Interface Paradigm: Spatial Hearing as an Informative Cue. PLoS One. 2010;5(3). doi:10.1371/journal.pone.0009813.</mixed-citation>
      </ref>
      <ref id="CR37">
        <label>37.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Schreuder</surname>
              <given-names>M</given-names>
            </name>
            <name>
              <surname>Rost</surname>
              <given-names>T</given-names>
            </name>
            <name>
              <surname>Tangermann</surname>
              <given-names>M</given-names>
            </name>
          </person-group>
          <article-title>Listen, you are writing! Speeding up online spelling with a dynamic auditory BCI</article-title>
          <source>Front Neurosci</source>
          <year>2011</year>
          <volume>5</volume>
          <fpage>112</fpage>
          <pub-id pub-id-type="doi">10.3389/fnins.2011.00112</pub-id>
          <pub-id pub-id-type="pmid">22016719</pub-id>
        </element-citation>
      </ref>
      <ref id="CR38">
        <label>38.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Simon</surname>
              <given-names>N</given-names>
            </name>
            <name>
              <surname>K&#xE4;thner</surname>
              <given-names>I</given-names>
            </name>
            <name>
              <surname>Ruf</surname>
              <given-names>CA</given-names>
            </name>
            <name>
              <surname>Pasqualotto</surname>
              <given-names>E</given-names>
            </name>
            <name>
              <surname>K&#xFC;bler</surname>
              <given-names>A</given-names>
            </name>
            <name>
              <surname>Halder</surname>
              <given-names>S</given-names>
            </name>
          </person-group>
          <article-title>An auditory multiclass brain-computer interface with natural stimuli: Usability evaluation with healthy participants and a motor impaired end user</article-title>
          <source>Front Hum Neurosci</source>
          <year>2015</year>
          <volume>8</volume>
          <fpage>1039</fpage>
          <pub-id pub-id-type="doi">10.3389/fnhum.2014.01039</pub-id>
          <pub-id pub-id-type="pmid">25620924</pub-id>
        </element-citation>
      </ref>
      <ref id="CR39">
        <label>39.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Schalk</surname>
              <given-names>G</given-names>
            </name>
            <name>
              <surname>McFarland</surname>
              <given-names>DJ</given-names>
            </name>
            <name>
              <surname>Hinterberger</surname>
              <given-names>T</given-names>
            </name>
            <name>
              <surname>Birbaumer</surname>
              <given-names>N</given-names>
            </name>
            <name>
              <surname>Wolpaw</surname>
              <given-names>JR</given-names>
            </name>
          </person-group>
          <article-title>BCI2000: A general-purpose, brain-computer interface (BCI) system</article-title>
          <source>IEEE Trans Biomed Eng</source>
          <year>2004</year>
          <volume>51</volume>
          <issue>6</issue>
          <fpage>1034</fpage>
          <lpage>43</lpage>
          <pub-id pub-id-type="doi">10.1109/TBME.2004.827072</pub-id>
          <pub-id pub-id-type="pmid">15188875</pub-id>
        </element-citation>
      </ref>
      <ref id="CR40">
        <label>40.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Krusienski</surname>
              <given-names>DJ</given-names>
            </name>
            <name>
              <surname>Sellers</surname>
              <given-names>EW</given-names>
            </name>
            <name>
              <surname>Cabestaing</surname>
              <given-names>F</given-names>
            </name>
            <name>
              <surname>Bayoudh</surname>
              <given-names>S</given-names>
            </name>
            <name>
              <surname>McFarland</surname>
              <given-names>DJ</given-names>
            </name>
            <name>
              <surname>Vaughan</surname>
              <given-names>TM</given-names>
            </name>
            <etal/>
          </person-group>
          <article-title>A comparison of classification techniques for the P300 Speller</article-title>
          <source>J Neural Eng</source>
          <year>2006</year>
          <volume>3</volume>
          <issue>4</issue>
          <fpage>299</fpage>
          <lpage>305</lpage>
          <pub-id pub-id-type="doi">10.1088/1741-2560/3/4/007</pub-id>
          <pub-id pub-id-type="pmid">17124334</pub-id>
        </element-citation>
      </ref>
      <ref id="CR41">
        <label>41.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Sharbrough</surname>
              <given-names>FW</given-names>
            </name>
            <name>
              <surname>Chatrian</surname>
              <given-names>G-E</given-names>
            </name>
            <name>
              <surname>Lesser</surname>
              <given-names>RP</given-names>
            </name>
            <name>
              <surname>L&#xFC;ders</surname>
              <given-names>H</given-names>
            </name>
            <name>
              <surname>Nuwer</surname>
              <given-names>M</given-names>
            </name>
            <name>
              <surname>Picton</surname>
              <given-names>TW</given-names>
            </name>
          </person-group>
          <article-title>American electroencephalographic society guidelines for standard electrode position nomenclature</article-title>
          <source>J Clin Neurophysiol</source>
          <year>1991</year>
          <volume>8</volume>
          <fpage>200</fpage>
          <lpage>2</lpage>
          <pub-id pub-id-type="doi">10.1097/00004691-199104000-00007</pub-id>
          <pub-id pub-id-type="pmid">2050819</pub-id>
        </element-citation>
      </ref>
      <ref id="CR42">
        <label>42.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Fried-Oken</surname>
              <given-names>M</given-names>
            </name>
            <name>
              <surname>Fox</surname>
              <given-names>L</given-names>
            </name>
            <name>
              <surname>Rau</surname>
              <given-names>MT</given-names>
            </name>
            <name>
              <surname>Tullman</surname>
              <given-names>J</given-names>
            </name>
            <name>
              <surname>Baker</surname>
              <given-names>G</given-names>
            </name>
            <name>
              <surname>Hindal</surname>
              <given-names>M</given-names>
            </name>
            <etal/>
          </person-group>
          <article-title>Purposes of AAC device use for persons with ALS as reported by caregivers</article-title>
          <source>Augment Altern Commun</source>
          <year>2006</year>
          <volume>22</volume>
          <issue>3</issue>
          <fpage>209</fpage>
          <lpage>21</lpage>
          <pub-id pub-id-type="doi">10.1080/07434610600650276</pub-id>
          <pub-id pub-id-type="pmid">17114164</pub-id>
        </element-citation>
      </ref>
      <ref id="CR43">
        <label>43.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Donegan</surname>
              <given-names>M</given-names>
            </name>
            <name>
              <surname>Morris</surname>
              <given-names>JD</given-names>
            </name>
            <name>
              <surname>Corno</surname>
              <given-names>F</given-names>
            </name>
            <name>
              <surname>Signorile</surname>
              <given-names>I</given-names>
            </name>
            <name>
              <surname>Chi&#xF3;</surname>
              <given-names>A</given-names>
            </name>
            <name>
              <surname>Pasian</surname>
              <given-names>V</given-names>
            </name>
            <etal/>
          </person-group>
          <article-title>Understanding users and their needs</article-title>
          <source>Univ Access Inf Soc</source>
          <year>2009</year>
          <volume>8</volume>
          <issue>4</issue>
          <fpage>259</fpage>
          <lpage>75</lpage>
          <pub-id pub-id-type="doi">10.1007/s10209-009-0148-1</pub-id>
        </element-citation>
      </ref>
      <ref id="CR44">
        <label>44.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Caligari</surname>
              <given-names>M</given-names>
            </name>
            <name>
              <surname>Godi</surname>
              <given-names>M</given-names>
            </name>
            <name>
              <surname>Guglielmetti</surname>
              <given-names>S</given-names>
            </name>
            <name>
              <surname>Franchignoni</surname>
              <given-names>F</given-names>
            </name>
            <name>
              <surname>Nardone</surname>
              <given-names>A</given-names>
            </name>
          </person-group>
          <article-title>Eye tracking communication devices in amyotrophic lateral sclerosis: Impact on disability and quality of life</article-title>
          <source>Amyotroph Lateral Scler Frontotemporal Degener</source>
          <year>2013</year>
          <volume>14</volume>
          <issue>7&#x2013;8</issue>
          <fpage>546</fpage>
          <lpage>52</lpage>
          <pub-id pub-id-type="doi">10.3109/21678421.2013.803576</pub-id>
          <pub-id pub-id-type="pmid">23834069</pub-id>
        </element-citation>
      </ref>
      <ref id="CR45">
        <label>45.</label>
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Calvo</surname>
              <given-names>A</given-names>
            </name>
            <name>
              <surname>Chi&#xF2;</surname>
              <given-names>A</given-names>
            </name>
            <name>
              <surname>Castellina</surname>
              <given-names>E</given-names>
            </name>
            <name>
              <surname>Corno</surname>
              <given-names>F</given-names>
            </name>
            <name>
              <surname>Farinetti</surname>
              <given-names>L</given-names>
            </name>
            <name>
              <surname>Ghiglione</surname>
              <given-names>P</given-names>
            </name>
            <etal/>
          </person-group>
          <article-title>Eye Tracking Impact on Quality-of-Life of ALS Patients</article-title>
          <source>11th International Conference on Computers Helping People with Special Needs, Linz (AT)</source>
          <year>2008</year>
          <fpage>70</fpage>
          <lpage>7</lpage>
        </element-citation>
      </ref>
      <ref id="CR46">
        <label>46.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Hwang</surname>
              <given-names>C-S</given-names>
            </name>
            <name>
              <surname>Weng</surname>
              <given-names>H-H</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>L-F</given-names>
            </name>
            <name>
              <surname>Tsai</surname>
              <given-names>C-H</given-names>
            </name>
            <name>
              <surname>Chang</surname>
              <given-names>H-T</given-names>
            </name>
          </person-group>
          <article-title>An Eye-Tracking Assistive Device Improves the Quality of Life for ALS Patients and Reduces the Caregivers&#x2019; Burden</article-title>
          <source>J Mot Behav</source>
          <year>2014</year>
          <volume>46</volume>
          <issue>4</issue>
          <fpage>233</fpage>
          <lpage>8</lpage>
          <pub-id pub-id-type="doi">10.1080/00222895.2014.891970</pub-id>
          <pub-id pub-id-type="pmid">24731126</pub-id>
        </element-citation>
      </ref>
      <ref id="CR47">
        <label>47.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Ball</surname>
              <given-names>LJ</given-names>
            </name>
            <name>
              <surname>Nordness</surname>
              <given-names>AS</given-names>
            </name>
            <name>
              <surname>Fager</surname>
              <given-names>SK</given-names>
            </name>
            <name>
              <surname>Kersch</surname>
              <given-names>K</given-names>
            </name>
            <name>
              <surname>Mohr</surname>
              <given-names>B</given-names>
            </name>
            <name>
              <surname>Pattee</surname>
              <given-names>GL</given-names>
            </name>
            <etal/>
          </person-group>
          <article-title>Eye-Gaze Access to AAC Technology for People with Amyotrophic Lateral Sclerosis</article-title>
          <source>J Med Speech-Lang Pathol</source>
          <year>2010</year>
          <volume>18</volume>
          <issue>3</issue>
          <fpage>11</fpage>
          <lpage>23</lpage>
        </element-citation>
      </ref>
      <ref id="CR48">
        <label>48.</label>
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Vilimek</surname>
              <given-names>R</given-names>
            </name>
            <name>
              <surname>Zander</surname>
              <given-names>TO</given-names>
            </name>
          </person-group>
          <person-group person-group-type="editor">
            <name>
              <surname>Stephanidis</surname>
              <given-names>C</given-names>
            </name>
          </person-group>
          <article-title>BC(eye): Combining Eye-Gaze Input with Brain-Computer Interaction</article-title>
          <source>Universal Access in Human-Computer Interaction Intelligent and Ubiquitous Interaction Environments</source>
          <year>2009</year>
          <publisher-loc>Berlin Heidelberg</publisher-loc>
          <publisher-name>Springer</publisher-name>
          <fpage>593</fpage>
          <lpage>602</lpage>
        </element-citation>
      </ref>
      <ref id="CR49">
        <label>49.</label>
        <mixed-citation publication-type="other">Baykara E, Ruf CA, Fioravanti C, K&#xE4;thner I, Simon N, Kleih SC, et al. Effects of training and motivation on auditory P300 brain-computer interface performance. Clinical Neurophysiology. 2015 (in press). doi:10.1016/j.clinph.2015.04.054</mixed-citation>
      </ref>
      <ref id="CR50">
        <label>50.</label>
        <mixed-citation publication-type="other">Halder S, K&#xE4;thner I, K&#xFC;bler A. Training leads to increased auditory brain-computer interface performance of end-users with motor impairments. Clinical Neurophysiology. 2015 (in press). doi:10.1016/j.clinph.2015.08.007</mixed-citation>
      </ref>
      <ref id="CR51">
        <label>51.</label>
        <mixed-citation publication-type="other">Marchetti M, Priftis K. Effectiveness of the P3-speller in brain&#x2013;computer interfaces for amyotrophic lateral sclerosis patients: a systematic review and meta-analysis. Front Neuroeng. 2014;7. doi:10.3389/fneng.2014.00012.</mixed-citation>
      </ref>
      <ref id="CR52">
        <label>52.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>K&#xE4;thner</surname>
              <given-names>I</given-names>
            </name>
            <name>
              <surname>K&#xFC;bler</surname>
              <given-names>A</given-names>
            </name>
            <name>
              <surname>Halder</surname>
              <given-names>S</given-names>
            </name>
          </person-group>
          <article-title>Rapid P300 brain-computer interface communication with a head-mounted display</article-title>
          <source>Front Neurosci</source>
          <year>2015</year>
          <volume>9</volume>
          <fpage>207</fpage>
          <pub-id pub-id-type="doi">10.3389/fnins.2015.00207</pub-id>
          <pub-id pub-id-type="pmid">26097447</pub-id>
        </element-citation>
      </ref>
      <ref id="CR53">
        <label>53.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>De Massari</surname>
              <given-names>D</given-names>
            </name>
            <name>
              <surname>Matuz</surname>
              <given-names>T</given-names>
            </name>
            <name>
              <surname>Furdea</surname>
              <given-names>A</given-names>
            </name>
            <name>
              <surname>Ruf</surname>
              <given-names>CA</given-names>
            </name>
            <name>
              <surname>Halder</surname>
              <given-names>S</given-names>
            </name>
            <name>
              <surname>Birbaumer</surname>
              <given-names>N</given-names>
            </name>
          </person-group>
          <article-title>Brain&#x2013;computer interface and semantic classical conditioning of communication in paralysis</article-title>
          <source>Biol Psychol</source>
          <year>2013</year>
          <volume>92</volume>
          <issue>2</issue>
          <fpage>267</fpage>
          <lpage>74</lpage>
          <pub-id pub-id-type="doi">10.1016/j.biopsycho.2012.10.015</pub-id>
          <pub-id pub-id-type="pmid">23153708</pub-id>
        </element-citation>
      </ref>
      <ref id="CR54">
        <label>54.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Murguialday</surname>
              <given-names>AR</given-names>
            </name>
            <name>
              <surname>Hill</surname>
              <given-names>J</given-names>
            </name>
            <name>
              <surname>Bensch</surname>
              <given-names>M</given-names>
            </name>
            <name>
              <surname>Martens</surname>
              <given-names>S</given-names>
            </name>
            <name>
              <surname>Halder</surname>
              <given-names>S</given-names>
            </name>
            <name>
              <surname>Nijboer</surname>
              <given-names>F</given-names>
            </name>
            <etal/>
          </person-group>
          <article-title>Transition from the locked in to the completely locked-in state: A physiological analysis</article-title>
          <source>Clin Neurophysiol</source>
          <year>2011</year>
          <volume>122</volume>
          <issue>5</issue>
          <fpage>925</fpage>
          <lpage>33</lpage>
          <pub-id pub-id-type="doi">10.1016/j.clinph.2010.08.019</pub-id>
          <pub-id pub-id-type="pmid">20888292</pub-id>
        </element-citation>
      </ref>
      <ref id="CR55">
        <label>55.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Naito</surname>
              <given-names>M</given-names>
            </name>
            <name>
              <surname>Michioka</surname>
              <given-names>Y</given-names>
            </name>
            <name>
              <surname>Ozawa</surname>
              <given-names>K</given-names>
            </name>
            <name>
              <surname>Ito</surname>
              <given-names>Y</given-names>
            </name>
            <name>
              <surname>Kiguchi</surname>
              <given-names>M</given-names>
            </name>
            <name>
              <surname>Kanazawa</surname>
              <given-names>T</given-names>
            </name>
          </person-group>
          <article-title>A communication means for totally locked-in ALS patients based on changes in cerebral blood volume measured with near-infrared light</article-title>
          <source>IEICE Trans Inf Syst</source>
          <year>2007</year>
          <volume>E90D</volume>
          <issue>7</issue>
          <fpage>1028</fpage>
          <lpage>37</lpage>
          <pub-id pub-id-type="doi">10.1093/ietisy/e90-d.7.1028</pub-id>
        </element-citation>
      </ref>
      <ref id="CR56">
        <label>56.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Mill&#xE1;n</surname>
              <given-names>JDR</given-names>
            </name>
            <name>
              <surname>Rupp</surname>
              <given-names>R</given-names>
            </name>
            <name>
              <surname>M&#xFC;ller-Putz</surname>
              <given-names>GR</given-names>
            </name>
            <name>
              <surname>Murray-Smith</surname>
              <given-names>R</given-names>
            </name>
            <name>
              <surname>Giugliemma</surname>
              <given-names>C</given-names>
            </name>
            <name>
              <surname>Tangermann</surname>
              <given-names>M</given-names>
            </name>
            <etal/>
          </person-group>
          <article-title>Combining brain&#x2013;computer interfaces and assistive technologies: state-of-the-art and challenges</article-title>
          <source>Front Neurosci</source>
          <year>2010</year>
          <volume>4</volume>
          <fpage>161</fpage>
          <pub-id pub-id-type="pmid">20877434</pub-id>
        </element-citation>
      </ref>
      <ref id="CR57">
        <label>57.</label>
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Holz</surname>
              <given-names>EM</given-names>
            </name>
            <name>
              <surname>Botrel</surname>
              <given-names>L</given-names>
            </name>
            <name>
              <surname>K&#xFC;bler</surname>
              <given-names>A</given-names>
            </name>
          </person-group>
          <person-group person-group-type="editor">
            <name>
              <surname>M&#xFC;ller-Putz</surname>
              <given-names>G</given-names>
            </name>
            <name>
              <surname>Bauernfeind</surname>
              <given-names>G</given-names>
            </name>
            <name>
              <surname>Brunner</surname>
              <given-names>C</given-names>
            </name>
            <name>
              <surname>Steryl</surname>
              <given-names>D</given-names>
            </name>
            <name>
              <surname>Wriessnegger</surname>
              <given-names>S</given-names>
            </name>
          </person-group>
          <article-title>Independent BCI Use in Two Patients Diagnosed with Amyotrophic Lateral Sclerosis</article-title>
          <source>Proceedings of the 6th International Brain-Computer Interface Conference</source>
          <year>2014</year>
          <fpage>92</fpage>
          <lpage>5</lpage>
        </element-citation>
      </ref>
      <ref id="CR58">
        <label>58.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>K&#xFC;bler</surname>
              <given-names>A</given-names>
            </name>
            <name>
              <surname>Holz</surname>
              <given-names>EM</given-names>
            </name>
            <name>
              <surname>Riccio</surname>
              <given-names>A</given-names>
            </name>
            <name>
              <surname>Zickler</surname>
              <given-names>C</given-names>
            </name>
            <name>
              <surname>Kaufmann</surname>
              <given-names>T</given-names>
            </name>
            <name>
              <surname>Kleih</surname>
              <given-names>SC</given-names>
            </name>
            <etal/>
          </person-group>
          <article-title>The user-centered design as novel perspective for evaluating the usability of BCI-controlled applications</article-title>
          <source>PLoS One</source>
          <year>2014</year>
          <volume>9</volume>
          <issue>12</issue>
          <fpage>e112392</fpage>
          <pub-id pub-id-type="doi">10.1371/journal.pone.0112392</pub-id>
          <pub-id pub-id-type="pmid">25469774</pub-id>
        </element-citation>
      </ref>
      <ref id="CR59">
        <label>59.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Zickler</surname>
              <given-names>C</given-names>
            </name>
            <name>
              <surname>Riccio</surname>
              <given-names>A</given-names>
            </name>
            <name>
              <surname>Leotta</surname>
              <given-names>F</given-names>
            </name>
            <name>
              <surname>Hillian-Tress</surname>
              <given-names>S</given-names>
            </name>
            <name>
              <surname>Halder</surname>
              <given-names>S</given-names>
            </name>
            <name>
              <surname>Holz</surname>
              <given-names>E</given-names>
            </name>
            <etal/>
          </person-group>
          <article-title>A Brain-Computer Interface as Input Channel for a Standard Assistive Technology Software</article-title>
          <source>Clin EEG Neurosci</source>
          <year>2011</year>
          <volume>42</volume>
          <issue>4</issue>
          <fpage>236</fpage>
          <lpage>44</lpage>
          <pub-id pub-id-type="doi">10.1177/155005941104200409</pub-id>
          <pub-id pub-id-type="pmid">22208121</pub-id>
        </element-citation>
      </ref>
      <ref id="CR60">
        <label>60.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Riccio</surname>
              <given-names>A</given-names>
            </name>
            <name>
              <surname>Leotta</surname>
              <given-names>F</given-names>
            </name>
            <name>
              <surname>Bianchi</surname>
              <given-names>L</given-names>
            </name>
            <name>
              <surname>Aloise</surname>
              <given-names>F</given-names>
            </name>
            <name>
              <surname>Zickler</surname>
              <given-names>C</given-names>
            </name>
            <name>
              <surname>Hoogerwerf</surname>
              <given-names>E-J</given-names>
            </name>
            <etal/>
          </person-group>
          <article-title>Workload measurement in a communication application operated through a P300-based brain&#x2013;computer interface</article-title>
          <source>J Neural Eng</source>
          <year>2011</year>
          <volume>8</volume>
          <issue>2</issue>
          <fpage>025028</fpage>
          <pub-id pub-id-type="doi">10.1088/1741-2560/8/2/025028</pub-id>
          <pub-id pub-id-type="pmid">21436511</pub-id>
        </element-citation>
      </ref>
    </ref-list>
  </back>
</article>
</pmc-articleset>
